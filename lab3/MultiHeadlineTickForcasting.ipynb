{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from Database import db\n",
    " \n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, concatenate, SpatialDropout1D, GRU\n",
    "from keras.layers import Dense, Flatten, Embedding, LSTM, Activation, BatchNormalization, Dropout, Conv1D, MaxPooling1D\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "import keras.backend as K\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Options\n",
    "\n",
    "stocks      = ['AMD', 'INTC']\n",
    "all_sources = ['reddit', 'reuters', 'twitter', 'seekingalpha', 'fool', 'wsj', 'thestreet']\n",
    "\n",
    "model_type  = 'multiheadline'\n",
    "\n",
    "test_cutoff = datetime(2018, 3, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78d0bf0da65344fe83fc02fd0563dbe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='AMD', max=1712), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c6c4523fd0943a7a0f4498534b9b27c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='INTC', max=2051), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Users\\Shriv\\Anaconda3\\envs\\tf-gpu\\lib\\site-packages\\gensim\\models\\doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def add_time(date, days):\n",
    "    \n",
    "    return (date + timedelta(days=days)).strftime('%Y-%m-%d')\n",
    "\n",
    "def clean(sentence):\n",
    "    \n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.replace('-', ' ').replace('_', ' ').replace('&', ' ')\n",
    "    sentence = ''.join(c for c in sentence if c in \"abcdefghijklmnopqrstuvwxyz \")\n",
    "    sentence = re.sub('\\s+', ' ', sentence)\n",
    "    \n",
    "    return sentence.strip()\n",
    "\n",
    "def make_doc_embeddings():\n",
    "\n",
    "    docs, labels = [], []\n",
    "    \n",
    "    class LabeledLineSentence:\n",
    "        \n",
    "        def __init__(self, docs, labels):\n",
    "            self.docs = docs\n",
    "            self.labels = labels\n",
    "            \n",
    "        def __iter__(self):\n",
    "            for idx, doc in enumerate(self.docs):\n",
    "                yield TaggedDocument(doc.split(), [self.labels[idx]])\n",
    "    \n",
    "    with db() as (conn, cur):\n",
    "        \n",
    "        for stock in stocks:\n",
    "            \n",
    "            ## Headline For Every Date ##\n",
    "            \n",
    "            cur.execute(\"SELECT DISTINCT date FROM headlines WHERE stock=? ORDER BY date ASC\", [stock])\n",
    "            dates = [date[0] for date in cur.fetchall()]\n",
    "            \n",
    "            for date in tqdm_notebook(dates, desc=stock):\n",
    "                \n",
    "                ## Collect Headlines ##\n",
    "                \n",
    "                event_date = datetime.strptime(date, '%Y-%m-%d')\n",
    "                \n",
    "                cur.execute(\"SELECT date, source, rawcontent FROM headlines WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date DESC\", \n",
    "                            [stock, add_time(event_date, -14), date])\n",
    "                headlines = [(date, source, clean(content), (event_date - datetime.strptime(date, '%Y-%m-%d')).days) \n",
    "                                 for (date, source, content) in cur.fetchall() if content]\n",
    "                \n",
    "                if len(headlines) < 2:\n",
    "                    continue\n",
    "                \n",
    "                ## Create training example ##\n",
    "                    \n",
    "                contents = [headline[2] for headline in headlines]\n",
    "\n",
    "                doc = \" \".join(contents)\n",
    "                \n",
    "                docs.append(doc)\n",
    "                labels.append(stock + \" \" + date)\n",
    "            \n",
    "    doc_iter = LabeledLineSentence(docs, labels)\n",
    "            \n",
    "    vec_model = Doc2Vec(documents=doc_iter,  \n",
    "                        size=300, \n",
    "                        window=10, \n",
    "                        min_count=5, \n",
    "                        workers=10,\n",
    "                        alpha=0.025, \n",
    "                        min_alpha=0.025, \n",
    "                        max_vocab_size=15000)\n",
    "    \n",
    "    vectors = {stock: {} for stock in stocks}\n",
    "    \n",
    "    for label in labels:\n",
    "        \n",
    "        stock, date = label.split(\" \")\n",
    "        \n",
    "        vectors[stock][date] = vec_model.docvecs[label]\n",
    "                    \n",
    "    return vec_model, (docs, labels)\n",
    "\n",
    "vec_model, vectors, data = make_doc_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('INTC 2016-04-17', 0.9361245036125183),\n",
       " ('INTC 2016-04-16', 0.9287127256393433),\n",
       " ('INTC 2016-04-19', 0.9262306690216064),\n",
       " ('INTC 2016-04-15', 0.92578125),\n",
       " ('INTC 2016-04-21', 0.9199097156524658),\n",
       " ('INTC 2016-04-18', 0.918441891670227),\n",
       " ('INTC 2016-04-22', 0.9165306091308594),\n",
       " ('AMD 2017-06-03', 0.9163079261779785),\n",
       " ('AMD 2017-05-18', 0.9153586626052856),\n",
       " ('AMD 2016-07-14', 0.9130065441131592)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_model.docvecs.most_similar(\"INTC 2016-04-20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAIN MODEL\n",
    "\n",
    "if __name__ == \"__main__\":  \n",
    "    \n",
    "    ## Save Tokenizer ##\n",
    "    \n",
    "    with open(os.path.join('..', 'models', 'toke2-tick.pkl'), 'wb') as toke_file:\n",
    "        pickle.dump(toke, toke_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    ## Create Model ##\n",
    "    \n",
    "    model = get_model(emb_matrix)\n",
    "    \n",
    "    monitor_mode = 'correct_sign_acc'\n",
    "    \n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(datetime.now().strftime(\"%Y,%m,%d-%H,%M,%S,tick,\" + model_type)))\n",
    "    e_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    checkpoint = ModelCheckpoint(os.path.join('..', 'models', 'media-headlines-ticks-' + model_type + '.h5'), \n",
    "                                 monitor=monitor_mode,\n",
    "                                 verbose=0,\n",
    "                                 save_best_only=True)\n",
    "    \n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    \n",
    "    ## Train ##\n",
    "    \n",
    "    history = model.fit([trainX, trainX2],\n",
    "                        trainY,\n",
    "                        epochs=epochs, \n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=([testX, testX2], testY),\n",
    "                        verbose=0,\n",
    "                        callbacks=[e_stopping, checkpoint, tensorboard])\n",
    "    \n",
    "    ## Display Train History ##\n",
    "    \n",
    "    plt.plot(np.log(history.history['loss']))\n",
    "    plt.plot(np.log(history.history['val_loss']))\n",
    "    plt.legend(['LogTrainLoss', 'LogTestLoss'])\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history[monitor_mode])\n",
    "    plt.plot(history.history['val_' + monitor_mode])\n",
    "    plt.legend(['TrainAcc', 'TestAcc'])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict (TEST)\n",
    "\n",
    "def predict(stock, model=None, toke=None, current_date=None, predict_date=None):\n",
    "    \n",
    "    import keras.metrics\n",
    "    keras.metrics.correct_sign_acc = correct_sign_acc\n",
    "    \n",
    "    if not model or not toke:\n",
    "        \n",
    "        with open(os.path.join('..', 'models', 'toke2-tick.pkl'), 'rb') as toke_file:\n",
    "            toke = pickle.load(toke_file)\n",
    "    \n",
    "        model = load_model(os.path.join('..', 'models', 'media-headlines-ticks-' + model_type + '.h5'))\n",
    "        \n",
    "    vocab_size = len(toke.word_counts)\n",
    "        \n",
    "    if not current_date:\n",
    "        current_date = datetime.today()\n",
    "        \n",
    "    if not predict_date:\n",
    "        predict_date = current_date + timedelta(days=1)\n",
    "    \n",
    "    all_headlines, all_tick_hist = [], []\n",
    "    \n",
    "    with db() as (conn, cur):\n",
    "        \n",
    "        event_date = current_date\n",
    "        date = datetime.strftime(event_date, '%Y-%m-%d')\n",
    "                \n",
    "        cur.execute(\"SELECT date, source, rawcontent FROM headlines WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date DESC\", \n",
    "                    [stock, add_time(event_date, -14), date])\n",
    "        headlines = [(date, source, clean(content), (event_date - datetime.strptime(date, '%Y-%m-%d')).days) \n",
    "                        for (date, source, content) in cur.fetchall() if content]\n",
    "                    \n",
    "        ## Find corresponding tick data ## \n",
    "                \n",
    "        cur.execute(\"\"\"SELECT open, high, low, adjclose, volume FROM ticks WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date DESC\"\"\", \n",
    "                    [stock, \n",
    "                     add_time(event_date, -30 - tick_window), \n",
    "                     add_time(event_date, 0)])\n",
    "                \n",
    "        before_headline_ticks = cur.fetchall()[:tick_window]\n",
    "        actual_current = before_headline_ticks[0][3]\n",
    "                \n",
    "        tick_hist = np.array(before_headline_ticks)\n",
    "        tick_hist -= np.mean(tick_hist, axis=0)\n",
    "        tick_hist /= np.std(tick_hist, axis=0)\n",
    "                \n",
    "        ## Create training example ##\n",
    "\n",
    "        probs = [1 / (headline[3] + 1) for headline in headlines]\n",
    "        probs /= np.sum(probs)\n",
    "                    \n",
    "        contents = [headline[2] for headline in headlines]\n",
    "\n",
    "        num_samples = len(contents) // sample_size\n",
    "\n",
    "        for i in range(num_samples):\n",
    "\n",
    "            indexes = np.random.choice(np.arange(len(headlines)), sample_size, replace=False, p=probs)\n",
    "                    \n",
    "            sample = [headlines[i] for i in indexes]\n",
    "\n",
    "            all_headlines.append(sample)\n",
    "            all_tick_hist.append(tick_hist)\n",
    "        \n",
    "        ## Process ##\n",
    "    \n",
    "        encoded_headlines, toke = encode_sentences(all_headlines, tokenizer=toke, max_length=max_length)\n",
    "        \n",
    "        tick_hists = np.array(all_tick_hist)\n",
    "        \n",
    "        predictions = model.predict([encoded_headlines, tick_hists])[:, 0]\n",
    "        \n",
    "        prices = predictions * 0.023 * actual_current + actual_current\n",
    "        \n",
    "        return predictions, prices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Change Coef: 0.26\n",
      "Predicting Price: 51.62\n",
      "Actual Price: 50.74\n"
     ]
    }
   ],
   "source": [
    "# [TEST] Spot Testing\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    ## **This Test May Overlap w/Train Data** ##\n",
    "    \n",
    "    ## Options ##\n",
    "    \n",
    "    stock = 'INTC'\n",
    "    current_date = '2018-03-07'\n",
    "    predict_date = '2018-03-08'\n",
    "    \n",
    "    ## Run ##\n",
    "    \n",
    "    predictions, prices = predict(stock, \n",
    "                                  current_date=datetime.strptime(current_date, '%Y-%m-%d'), \n",
    "                                  predict_date=datetime.strptime(predict_date, '%Y-%m-%d'))\n",
    "    \n",
    "    ## Find Actual Value ##\n",
    "     \n",
    "    with db() as (conn, cur):\n",
    "    \n",
    "        cur.execute(\"\"\"SELECT adjclose FROM ticks WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date ASC LIMIT 1\"\"\", \n",
    "                        [stock, \n",
    "                        add_time(datetime.strptime(predict_date, '%Y-%m-%d'), 0), \n",
    "                        add_time(datetime.strptime(predict_date, '%Y-%m-%d'), 6)])\n",
    "\n",
    "        after_headline_ticks = cur.fetchall()\n",
    "        try:\n",
    "            actual_result = after_headline_ticks[0][0]\n",
    "        except:\n",
    "            actual_result = -1\n",
    "            \n",
    "    ## Display ##\n",
    "            \n",
    "    parse = lambda num: str(round(num, 2))\n",
    "    \n",
    "    print(\"Predicting Change Coef: \" + parse(np.mean(predictions)))\n",
    "    print(\"Predicting Price: \" + parse(np.mean(prices)))\n",
    "    print(\"Actual Price: \" + parse(actual_result))\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-gpu]",
   "language": "python",
   "name": "conda-env-tf-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
