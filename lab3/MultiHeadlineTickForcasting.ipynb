{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import warnings; warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from Database import db\n",
    " \n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, concatenate, SpatialDropout1D, GRU\n",
    "from keras.layers import Dense, Flatten, Embedding, LSTM, Activation, BatchNormalization, Dropout, Conv1D, MaxPooling1D\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "import keras.backend as K\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Options\n",
    "\n",
    "stocks      = ['AMD', 'INTC', 'AAPL', 'AMZN', 'MSFT', 'GOOG']\n",
    "all_sources = ['reddit', 'reuters', 'twitter', 'seekingalpha', 'fool', 'wsj', 'thestreet']\n",
    "\n",
    "model_type  = 'multiheadlineclf'\n",
    "\n",
    "doc2vec_options = dict(\n",
    "    size=300, \n",
    "    window=10, \n",
    "    min_count=5, \n",
    "    workers=10,\n",
    "    alpha=0.025, \n",
    "    min_alpha=0.025, \n",
    "    max_vocab_size=13000,\n",
    "    dm=1\n",
    ")\n",
    "\n",
    "keras_options = dict(\n",
    "    epochs=200, \n",
    "    batch_size=32,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "tick_window = 12\n",
    "doc_query_days = 10\n",
    "combined_emb_size = 5 + doc2vec_options['size']\n",
    "\n",
    "test_cutoff = datetime(2018, 4, 12) # TODO use this for train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def add_time(date, days):\n",
    "    \n",
    "    return (date + timedelta(days=days)).strftime('%Y-%m-%d')\n",
    "\n",
    "def clean(sentence):\n",
    "\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.replace('-', ' ').replace('_', ' ').replace('&', ' ')\n",
    "    sentence = ''.join(char for char in sentence if char in \"abcdefghijklmnopqrstuvwxyz.!? \")\n",
    "    sentence = re.sub('\\s+', ' ', sentence).strip()\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "def make_doc_embeddings(query_range=('1776-07-04', '3000-01-01'), use_extra_dates=True, vec_model=None):\n",
    "    \"\"\"\n",
    "    Create document embeddings from headlines\n",
    "    \"\"\"\n",
    "    print('Creating doc embeddings...')\n",
    "\n",
    "    docs, labels = [], []\n",
    "    \n",
    "    class LabeledLineSentence:\n",
    "        \n",
    "        def __init__(self, docs, labels):\n",
    "            self.docs = docs\n",
    "            self.labels = labels\n",
    "            \n",
    "        def __iter__(self):\n",
    "            for idx, doc in enumerate(self.docs):\n",
    "                yield TaggedDocument(doc.split(), [self.labels[idx]]) # clean doc\n",
    "    \n",
    "    with db() as (conn, cur):\n",
    "        \n",
    "        for stock in stocks:\n",
    "            \n",
    "            ## Headline For Every Date ##\n",
    "            \n",
    "            q_start, q_end = query_range\n",
    "            \n",
    "            cur.execute(\"SELECT DISTINCT date FROM headlines WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date ASC\", [stock, q_start, q_end])\n",
    "            dates = [date[0] for date in cur.fetchall()]\n",
    "            \n",
    "            if use_extra_dates: # True headline days not enough so we create additional querys\n",
    "                new_dates = []\n",
    "                for date in dates: \n",
    "                    d = datetime.strptime(date, '%Y-%m-%d')\n",
    "                    new_dates.append(add_time(d, -1))\n",
    "                    new_dates.append(add_time(d, +1))\n",
    "                dates.extend(new_dates)\n",
    "            \n",
    "            for date in tqdm_notebook(dates, desc=stock):\n",
    "                \n",
    "                ## Collect Headlines ##\n",
    "                \n",
    "                event_date = datetime.strptime(date, '%Y-%m-%d')\n",
    "                \n",
    "                cur.execute(\"SELECT date, source, rawcontent FROM headlines WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date ASC\", \n",
    "                            [stock, add_time(event_date, -doc_query_days), date])\n",
    "                headlines = [(date, source, clean(content), (event_date - datetime.strptime(date, '%Y-%m-%d')).days) \n",
    "                                 for (date, source, content) in cur.fetchall() if content]\n",
    "                \n",
    "                if len(headlines) == 0:\n",
    "                    continue\n",
    "                \n",
    "                ## Create training example ##\n",
    "                    \n",
    "                contents = [headline[2] for headline in headlines]\n",
    "\n",
    "                doc = \" **NEXT** \".join(contents)\n",
    "                \n",
    "                docs.append(doc)\n",
    "                labels.append(stock + \" \" + date)\n",
    "                \n",
    "    vectors = {stock: {} for stock in stocks}\n",
    "            \n",
    "    doc_iter = LabeledLineSentence(docs, labels)\n",
    "    \n",
    "    if not vec_model:\n",
    "        \n",
    "        vec_model = Doc2Vec(documents=doc_iter, **doc2vec_options)\n",
    "        #     vec_model = Doc2Vec(**doc2vec_options)\n",
    "        #     vec_model.build_vocab(doc_iter)\n",
    "\n",
    "        #     for epoch in range(100):\n",
    "        #         vec_model.train(doc_iter, **doc2vec_options)\n",
    "        #         vec_model.alpha -= 0.002\n",
    "        #         vec_model.min_alpha = vec_model.alpha\n",
    "        \n",
    "        for label in labels:\n",
    "        \n",
    "            stock, date = label.split(\" \")\n",
    "\n",
    "            vectors[stock][date] = vec_model.docvecs[label]\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        for tag_doc in doc_iter:\n",
    "            \n",
    "            vec = vec_model.infer_vector(tag_doc.words, \n",
    "                                         alpha=doc2vec_options['alpha'], \n",
    "                                         min_alpha=doc2vec_options['min_alpha'], \n",
    "                                         steps=1000)\n",
    "            \n",
    "            stock, date = tag_doc.tags[0].split(\" \")\n",
    "            \n",
    "            vectors[stock][date] = vec\n",
    "            \n",
    "    return vec_model, vectors, (docs, labels)\n",
    "\n",
    "def make_tick_data(query_range=('1776-07-04', '3000-01-01')):\n",
    "    \"\"\"\n",
    "    Process historic tick data (high/low/close/etc..) into training examples\n",
    "    \"\"\"\n",
    "    print('Creating tick data...')\n",
    "    \n",
    "    tick_vecs = {stock: {} for stock in stocks}\n",
    "    effect_vecs = {stock: {} for stock in stocks}\n",
    "    \n",
    "    with db() as (conn, cur):\n",
    "        \n",
    "        for stock in stocks:\n",
    "            \n",
    "            q_start, q_end = query_range\n",
    "            \n",
    "            cur.execute(\"SELECT DISTINCT date FROM headlines WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date ASC LIMIT 1\", [stock, q_start, q_end])\n",
    "            start_date = cur.fetchall()[0][0]\n",
    "            \n",
    "            cur.execute(\"SELECT DISTINCT date FROM ticks WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date ASC\", [stock, start_date, q_end])\n",
    "            dates = [date[0] for date in cur.fetchall()]\n",
    "            \n",
    "            for date in dates:\n",
    "                \n",
    "                event_date = datetime.strptime(date, '%Y-%m-%d') # The date of headline\n",
    "\n",
    "                ## Find corresponding tick data ## \n",
    "\n",
    "                cur.execute(\"\"\"SELECT open, high, low, adjclose, volume FROM ticks WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date DESC LIMIT 52\"\"\", \n",
    "                            [stock, \n",
    "                             add_time(event_date, -80), \n",
    "                             add_time(event_date, 0)])\n",
    "\n",
    "                before_headline_ticks = cur.fetchall()\n",
    "\n",
    "                if len(before_headline_ticks) < tick_window:\n",
    "                    continue\n",
    "\n",
    "                cur.execute(\"\"\"SELECT adjclose FROM ticks WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date ASC LIMIT 1\"\"\", \n",
    "                            [stock, \n",
    "                            add_time(event_date, 1), \n",
    "                            add_time(event_date, 4)])\n",
    "\n",
    "                after_headline_ticks = cur.fetchall()\n",
    "\n",
    "                ## Create ##\n",
    "\n",
    "                if len(after_headline_ticks) == 0:\n",
    "                    continue\n",
    "\n",
    "                window_ticks = np.array(list(reversed(before_headline_ticks[:tick_window]))) # Flip so in chron. order\n",
    "                fifty_ticks = np.array(before_headline_ticks) # Use last 50 ticks to normalize\n",
    "\n",
    "                previous_tick = before_headline_ticks[0][3]\n",
    "                result_tick = after_headline_ticks[0][0]\n",
    "\n",
    "                if previous_tick and result_tick:\n",
    "\n",
    "                    window_ticks -= np.mean(fifty_ticks, axis=0)\n",
    "                    window_ticks /= np.std(fifty_ticks, axis=0)\n",
    "                    \n",
    "                    if result_tick > previous_tick:\n",
    "                        effect = [1., 0.]\n",
    "                    else:\n",
    "                        effect = [0., 1.]\n",
    "                    \n",
    "                    tick_vecs[stock][date] = window_ticks\n",
    "                    effect_vecs[stock][date] = effect\n",
    "                    \n",
    "    return tick_vecs, effect_vecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def merge_data(doc_vecs, tick_vecs, effect_vecs):\n",
    "    \"\"\"\n",
    "    Pairs document and tick vectors (both timeseries) to an effect vector (up/down)\n",
    "    \"\"\"\n",
    "    print('Creating X, Y...')\n",
    "    \n",
    "    X, Y, test_indices = [], [], []\n",
    "    \n",
    "    for stock in stocks:\n",
    "        \n",
    "        for date, tick_vec in tick_vecs[stock].items():\n",
    "            \n",
    "            x = []\n",
    "            y = effect_vecs[stock][date]\n",
    "            \n",
    "            event_date = datetime.strptime(date, '%Y-%m-%d')\n",
    "            \n",
    "            window_dates = [add_time(event_date, -i) for i in range(tick_window)]\n",
    "            \n",
    "            for i in range(tick_window):\n",
    "                \n",
    "                if window_dates[i] not in doc_vecs[stock]:\n",
    "                    break\n",
    "                    \n",
    "                x_i = np.concatenate([tick_vec[i], doc_vecs[stock][window_dates[i]]]) # Combine tick data and doc data\n",
    "                \n",
    "                x.append(x_i)\n",
    "                \n",
    "            if len(x) == tick_window:\n",
    "                \n",
    "                X.append(x)\n",
    "                Y.append(y)\n",
    "                \n",
    "                if event_date > test_cutoff: # Label as test data\n",
    "                    test_indices.append(len(X) - 1)\n",
    "        \n",
    "    return np.array(X), np.array(Y), np.array(test_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def split_data(X, Y, test_indices):\n",
    "    \"\"\"\n",
    "    Splits X/Y to Train/Test\n",
    "    \"\"\"\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    train_indices = np.setdiff1d(indices, test_indices, assume_unique=True)\n",
    "    \n",
    "    trainX,  testX  = X[train_indices],  X[test_indices]\n",
    "    trainY,  testY  = Y[train_indices],  Y[test_indices]\n",
    "    \n",
    "    return trainX, trainY, testX, testY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def correct_sign_acc(y_true, y_pred): # Currently not used\n",
    "    \"\"\"\n",
    "    Accuracy of Being Positive or Negative\n",
    "    \"\"\"\n",
    "    diff = K.equal(y_true > 0, y_pred > 0)\n",
    "    \n",
    "    return K.mean(diff, axis=-1)\n",
    "\n",
    "def get_model():\n",
    "    \n",
    "    model_input = Input(shape=(tick_window, combined_emb_size), name=\"Input\")\n",
    "    \n",
    "    rnn = LSTM(500, return_sequences=False)(model_input)\n",
    "    rnn = Dropout(0.3)(rnn)\n",
    "    \n",
    "    dense = Dense(500)(rnn)\n",
    "    dense = Activation('selu')(dense)\n",
    "    dense = BatchNormalization()(dense)\n",
    "    dense = Dropout(0.3)(dense)\n",
    "    \n",
    "    dense = Dense(500)(dense)\n",
    "    dense = Activation('selu')(dense)\n",
    "    dense = BatchNormalization()(dense)\n",
    "    dense = Dropout(0.3)(dense)\n",
    "    \n",
    "    dense = Dense(2)(dense)\n",
    "    pred_output = Activation('softmax')(dense)\n",
    "    \n",
    "    model = Model(inputs=model_input, outputs=pred_output)\n",
    "    \n",
    "    model.compile(optimizer=Adam(), loss='mse', metrics=['acc'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    vec_model, doc_vecs, doc_data = make_doc_embeddings() #vec_model.docvecs.most_similar(\"INTC 2016-04-20\")\n",
    "    \n",
    "    tick_vecs, effect_vecs = make_tick_data()\n",
    "    \n",
    "    X, Y, test_indices = merge_data(doc_vecs, tick_vecs, effect_vecs)\n",
    "    \n",
    "    trainX, trainY, testX, testY = split_data(X, Y, test_indices)\n",
    "    \n",
    "    print(trainX.shape, testY.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN MODEL\n",
    "\n",
    "if __name__ == \"__main__\":  \n",
    "\n",
    "    ## Create Model ##\n",
    "    \n",
    "    model = get_model()\n",
    "    \n",
    "    monitor_mode = 'acc'\n",
    "    \n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(datetime.now().strftime(\"%Y,%m,%d-%H,%M,%S,tick,\" + model_type)))\n",
    "    e_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    checkpoint = ModelCheckpoint(os.path.join('..', 'models', 'media-headlines-ticks-' + model_type + '.h5'), \n",
    "                                 monitor=monitor_mode,\n",
    "                                 verbose=0,\n",
    "                                 save_best_only=True)\n",
    "    \n",
    "    vec_model.save(os.path.join('..', 'models', 'doc2vec-' + model_type + '.doc2vec'))\n",
    "    \n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    \n",
    "    ## Train ##\n",
    "    \n",
    "    history = model.fit(trainX,\n",
    "                        trainY,\n",
    "                        validation_data=(testX, testY),\n",
    "                        callbacks=[e_stopping, tensorboard, checkpoint],\n",
    "                        **keras_options)\n",
    "    \n",
    "    ## Display Train History ##\n",
    "    \n",
    "    plt.plot(np.log(history.history['loss']))\n",
    "    plt.plot(np.log(history.history['val_loss']))\n",
    "    plt.legend(['LogTrainLoss', 'LogTestLoss'])\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history[monitor_mode])\n",
    "    plt.plot(history.history['val_' + monitor_mode])\n",
    "    plt.legend(['TrainAcc', 'TestAcc'])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AoC\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        actualY = testY\n",
    "        predictY = model.predict(testX)\n",
    "        \n",
    "        print(\"ROC\", roc_auc_score(actualY, predictY))\n",
    "        \n",
    "        print(confusion_matrix(testY[:, 0] > .7, predictY[:, 0] > .7))\n",
    "        \n",
    "    except NameError:\n",
    "        \n",
    "        print(\"Test Data and Model Required!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict (TEST)\n",
    "\n",
    "def predict(stock, model=None, vec_model=None, current_date=None, predict_date=None):\n",
    "    \n",
    "    if not model or not vec_model:\n",
    "        \n",
    "        vec_model = Doc2Vec.load(os.path.join('..', 'models', 'doc2vec-' + model_type + '.doc2vec'))\n",
    "    \n",
    "        model = load_model(os.path.join('..', 'models', 'media-headlines-ticks-' + model_type + '.h5'))\n",
    "        \n",
    "    if not current_date:\n",
    "        current_date = datetime.today()\n",
    "        \n",
    "    if not predict_date:\n",
    "        predict_date = current_date + timedelta(days=1)\n",
    "    \n",
    "    ## PREDICT HERE ##\n",
    "        \n",
    "    return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # [TEST] Spot Testing\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "    \n",
    "#     ## **This Test May Overlap w/Train Data** ##\n",
    "    \n",
    "#     ## Options ##\n",
    "    \n",
    "#     stock = 'INTC'\n",
    "#     current_date = '2018-03-07'\n",
    "#     predict_date = '2018-03-08'\n",
    "    \n",
    "#     ## Run ##\n",
    "    \n",
    "#     predictions, prices = predict(stock, \n",
    "#                                   current_date=datetime.strptime(current_date, '%Y-%m-%d'), \n",
    "#                                   predict_date=datetime.strptime(predict_date, '%Y-%m-%d'))\n",
    "    \n",
    "#     ## Find Actual Value ##\n",
    "     \n",
    "#     with db() as (conn, cur):\n",
    "    \n",
    "#         cur.execute(\"\"\"SELECT adjclose FROM ticks WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date ASC LIMIT 1\"\"\", \n",
    "#                         [stock, \n",
    "#                         add_time(datetime.strptime(predict_date, '%Y-%m-%d'), 0), \n",
    "#                         add_time(datetime.strptime(predict_date, '%Y-%m-%d'), 6)])\n",
    "\n",
    "#         after_headline_ticks = cur.fetchall()\n",
    "#         try:\n",
    "#             actual_result = after_headline_ticks[0][0]\n",
    "#         except:\n",
    "#             actual_result = -1\n",
    "            \n",
    "#     ## Display ##\n",
    "            \n",
    "#     parse = lambda num: str(round(num, 2))\n",
    "    \n",
    "#     print(\"Predicting Change Coef: \" + parse(np.mean(predictions)))\n",
    "#     print(\"Predicting Price: \" + parse(np.mean(prices)))\n",
    "#     print(\"Actual Price: \" + parse(actual_result))\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-gpu]",
   "language": "python",
   "name": "conda-env-tf-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
