{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup (Imports)\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "\n",
    "import requests\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "\n",
    "from Database import add_stock_ticks, add_headlines, clean_ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def consume_ticker_csv(stock, filename):\n",
    "    \"\"\"Loads data from csv file into database\"\"\"\n",
    "    entries = []\n",
    "    \n",
    "    with open(os.path.join('..', 'data', filename), 'r') as tick_csv:\n",
    "        \n",
    "        for line in tick_csv:\n",
    "            \n",
    "            if \"Date\" not in line:\n",
    "                \n",
    "                date, open_, high, low, close, adj_close, volume = line.split(',')\n",
    "                \n",
    "                entries.append((stock, date, open_, high, low, close, adj_close, volume))\n",
    "                \n",
    "    add_stock_ticks(entries)\n",
    "    \n",
    "    clean_ticks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_reddit_news(subs, search_terms, limit=None, praw_config='StockMarketML'):\n",
    "    \"Get headlines from Reddit\"\n",
    "    print('Downloading Reddit Posts: ' + \", \".join(subs))\n",
    "    \n",
    "    from praw import Reddit\n",
    "    \n",
    "    reddit = Reddit(praw_config)\n",
    "\n",
    "    articles = defaultdict(list)\n",
    "    \n",
    "    used = []\n",
    "    \n",
    "    for term in search_terms:\n",
    "\n",
    "        for submission in reddit.subreddit('+'.join(subs)).search(term, limit=limit):\n",
    "            \n",
    "            if submission.title.count(' ') > 4 and submission.title not in used:\n",
    "                \n",
    "                used.append(submission.title)\n",
    "                \n",
    "                date_key = datetime.fromtimestamp(submission.created).strftime('%Y-%m-%d')\n",
    "\n",
    "                articles[date_key].append(submission.title)\n",
    "        \n",
    "    return articles\n",
    "\n",
    "def get_reuters_news(stock, pages=80):\n",
    "    \"\"\"Get headlines from Reuters\"\"\"\n",
    "    print('Downloading Reuters: ' + stock)\n",
    "    \n",
    "    found_headlines = []\n",
    "    \n",
    "    articles = defaultdict(list)\n",
    "    \n",
    "    pattern_headline = re.compile('<h2><a [\\s\\S]+?>([\\s\\S]+?)<\\/a>[\\s\\S]*?<\\/h2>')\n",
    "    \n",
    "    date_current = datetime.now()\n",
    "    \n",
    "    while pages > 0:\n",
    "\n",
    "        text = requests.get('http://www.reuters.com/finance/stocks/company-news/{}?date={}'.format(stock, date_current.strftime('%m%d%Y')),  headers={'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36'}).text\n",
    "        \n",
    "        for match in pattern_headline.finditer(text):\n",
    "            \n",
    "            headline = match.group(1)\n",
    "            \n",
    "            headline = headline.replace('\\u200d', '').replace('\\u200b', '')\n",
    "            \n",
    "            headline = re.sub('^[A-Z]+[A-Z\\d\\s]*\\-', '', headline)\n",
    "            \n",
    "            date_key = date_current.strftime('%Y-%m-%d')\n",
    "            \n",
    "            if headline not in found_headlines:\n",
    "            \n",
    "                articles[date_key].append(headline)\n",
    "                found_headlines.append(headline)\n",
    "        \n",
    "        pages -= 1\n",
    "        \n",
    "        date_current -= timedelta(days=1)\n",
    "        \n",
    "    return articles\n",
    "\n",
    "def get_twitter_news(querys, limit=100):\n",
    "    \"\"\"Get headlines from Twitter\"\"\"\n",
    "    print('Downloading Tweets: ' + \", \".join(querys))\n",
    "    \n",
    "    from twitter import Twitter, OAuth\n",
    "    import twitter_creds as c # Self-Created Python file with Creds\n",
    "\n",
    "    twitter = Twitter(auth=OAuth(c.ACCESS_TOKEN, c.ACCESS_SECRET, c.CONSUMER_KEY, c.CONSUMER_SECRET))\n",
    "    \n",
    "    limit = min(limit, 100)\n",
    "    \n",
    "    articles = defaultdict(list)\n",
    "    \n",
    "    for query in querys:\n",
    "    \n",
    "        tweets = twitter.search.tweets(q=query, result_type='popular', lang='en', count=limit)['statuses']\n",
    "        \n",
    "        for tweet in tweets:\n",
    "            \n",
    "            text = re.sub(r'https?:\\/\\/\\S+', '', tweet['text'])\n",
    "            text = re.sub(r'[^\\w\\s:/]+', '', text)\n",
    "            \n",
    "            date = tweet['created_at']\n",
    "            \n",
    "            if '\\n' not in text and len(text) > len(query) and ' ' in text:\n",
    "                \n",
    "                date_key = datetime.strptime(date, \"%a %b %d %H:%M:%S %z %Y\" ).strftime('%Y-%m-%d')\n",
    "                \n",
    "                articles[date_key].append(text)\n",
    "                \n",
    "    return articles\n",
    "\n",
    "def get_seekingalpha_news(stock, pages=500):\n",
    "    \"\"\"Get headlines from SeekingAlpha\"\"\"\n",
    "    print('Downloading SeekingAlpha: ' + stock)\n",
    "\n",
    "    articles = defaultdict(list)\n",
    "\n",
    "    re_headline = re.compile('<a class=\"market_current_title\" [\\s\\S]+?>([\\s\\S]+?)<\\/a>')\n",
    "    re_dates = re.compile('<span class=\"date pad_on_summaries\">([\\s\\S]+?)<\\/span>')\n",
    "\n",
    "    cookies = None\n",
    "\n",
    "    for i in range(1, pages + 1):\n",
    "\n",
    "        if i == 1:\n",
    "            url = 'https://seekingalpha.com/symbol/{}/news'.format(stock)\n",
    "        else:\n",
    "            url = 'https://seekingalpha.com/symbol/{}/news/more_news_all?page={}'.format(stock, i)\n",
    "            \n",
    "        try:\n",
    "\n",
    "            r = requests.get(url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36'}, cookies=cookies)\n",
    "        \n",
    "        except Exception as e:\n",
    "            \n",
    "            print(e)\n",
    "            continue\n",
    "    \n",
    "        text = r.text.replace('\\\\\"', '\"')\n",
    "        cookies = r.cookies # SeekingAlpha wants cookies.\n",
    "\n",
    "        headlines = [match.group(1) for match in re_headline.finditer(text)]\n",
    "        dates = [match.group(1) for match in re_dates.finditer(text)]\n",
    "\n",
    "        for headline, date in zip(headlines, dates):\n",
    "            \n",
    "            headline = headline.replace('(update)', '')\n",
    "            \n",
    "            date = date.replace('.', '')\n",
    "\n",
    "            if 'Today' in date:\n",
    "                date = datetime.today()\n",
    "            elif 'Yesterday' in date:\n",
    "                date = datetime.today() - timedelta(days=1)\n",
    "            else:\n",
    "                temp = date.split(',')\n",
    "                if len(temp[0]) == 3:\n",
    "                    date = datetime.strptime(temp[1], \" %b %d\").replace(year=datetime.today().year)\n",
    "                else:\n",
    "                    date = datetime.strptime(\"\".join(temp[0:2]), \"%b %d %Y\")\n",
    "\n",
    "            articles[date.strftime('%Y-%m-%d')].append(headline)\n",
    "\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_headline(headline, replacements={}):\n",
    "    \"\"\"\n",
    "    Clean headline\n",
    "    \n",
    "    Removes extra chars and replaces words\n",
    "    \"\"\"\n",
    "    headline = headline.lower()\n",
    "    headline = re.sub('\\d+%', 'STAT', headline)\n",
    "    headline = ''.join(c for c in headline if c in \"abcdefghijklmnopqrstuvwxyz \")\n",
    "    headline = re.sub('\\s+', ' ', headline)\n",
    "    \n",
    "    for original, replacement in replacements.items():\n",
    "        headline = headline.replace(original, replacement)\n",
    "        \n",
    "    headline = headline.replace('STAT', '**STATISTIC**')\n",
    "        \n",
    "    headline = headline.replace('****', '** **') # Seperate joined kwords\n",
    "    \n",
    "    return headline.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def save_headlines(headlines, kword_replacements={}):\n",
    "    \"\"\"Save headlines to file\"\"\"\n",
    "    \n",
    "    for stock in headlines:\n",
    "        \n",
    "        entries = []\n",
    "        \n",
    "        for source in headlines[stock]:\n",
    "            \n",
    "            for date in headlines[stock][source]:\n",
    "                \n",
    "                for headline in headlines[stock][source][date]:\n",
    "                    \n",
    "                    headline = clean_headline(headline, kword_replacements[stock])\n",
    "                    \n",
    "                    entries.append((stock, date, source, headline))\n",
    "                    \n",
    "        add_headlines(entries)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Reddit Posts: google, Android, GooglePixel, news\n",
      "Downloading Reuters: GOOG.O\n",
      "Downloading Tweets: @Google, #Google, #googlepixel, #Alphabet\n",
      "Downloading SeekingAlpha: GOOG\n",
      "Downloading Reddit Posts: apple, ios, AAPL, news\n",
      "Downloading Reuters: AAPL.O\n",
      "Downloading Tweets: @Apple, #Apple, #IPhone, #ios\n",
      "Downloading SeekingAlpha: AAPL\n",
      "Downloading Reddit Posts: microsoft, windowsphone, windows\n",
      "Downloading Reuters: MSFT.O\n",
      "Downloading Tweets: @Microsoft, #Windows, #Microsoft, #windowsphone\n",
      "Downloading SeekingAlpha: MSFT\n",
      "Downloading Reddit Posts: Amd, AMD_Stock, pcmasterrace\n",
      "Downloading Reuters: AMD.O\n",
      "Downloading Tweets: @AMD, #AMD, #Ryzen, #radeon\n",
      "Downloading SeekingAlpha: AMD\n",
      "Downloading Reddit Posts: amazon, amazonprime, amazonecho\n",
      "Downloading Reuters: AMZN.O\n",
      "Downloading Tweets: @amazon, #Amazon, #jeffbezos, @amazonecho, #amazonprime\n",
      "Downloading SeekingAlpha: AMZN\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    headlines = {\n",
    "            'GOOG': {\n",
    "                'reddit': get_reddit_news(['google', 'Android', 'GooglePixel', 'news'], ['Google', 'pixel', 'android', 'stock']), \n",
    "                'reuters': get_reuters_news('GOOG.O'),\n",
    "                'twitter': get_twitter_news(['@Google', '#Google', '#googlepixel', '#Alphabet']),\n",
    "                'seekingalpha': get_seekingalpha_news('GOOG')\n",
    "            },\n",
    "            'AAPL': {\n",
    "                'reddit': get_reddit_news(['apple', 'ios', 'AAPL', 'news'], ['apple', 'iphone', 'ipad', 'ios', 'stock']), \n",
    "                'reuters': get_reuters_news('AAPL.O'),\n",
    "                'twitter': get_twitter_news(['@Apple', '#Apple', '#IPhone', '#ios']),\n",
    "                'seekingalpha': get_seekingalpha_news('AAPL')\n",
    "            },\n",
    "            'MSFT': {\n",
    "                'reddit': get_reddit_news(['microsoft', 'windowsphone', 'windows'], ['microsoft', 'phone', 'windows', 'stock']), \n",
    "                'reuters': get_reuters_news('MSFT.O'),\n",
    "                'twitter': get_twitter_news(['@Microsoft', '#Windows', '#Microsoft', '#windowsphone']),\n",
    "                'seekingalpha': get_seekingalpha_news('MSFT')\n",
    "            },\n",
    "            'AMD': {\n",
    "                'reddit': get_reddit_news(['Amd', 'AMD_Stock', 'pcmasterrace'], ['AMD', 'radeon', 'ryzen', 'stock']), \n",
    "                'reuters': get_reuters_news('AMD.O'),\n",
    "                'twitter': get_twitter_news(['@AMD', '#AMD', '#Ryzen', '#radeon']),\n",
    "                'seekingalpha': get_seekingalpha_news('AMD')\n",
    "            },\n",
    "            'AMZN': {\n",
    "                'reddit': get_reddit_news(['amazon', 'amazonprime', 'amazonecho'], ['amazon', 'echo', 'prime', 'stock']), \n",
    "                'reuters': get_reuters_news('AMZN.O'),\n",
    "                'twitter': get_twitter_news(['@amazon', '#Amazon', '#jeffbezos', '@amazonecho', '#amazonprime']),\n",
    "                'seekingalpha': get_seekingalpha_news('AMZN')\n",
    "            }\n",
    "    }\n",
    "    \n",
    "    kword_replacements = { # To futher generalize headlines\n",
    "        'GOOG': {\n",
    "            'googleplay': '**PRODUCT**',\n",
    "            'googlemusic': '**PRODUCT**',\n",
    "            'googlehome': '**PRODUCT**',\n",
    "            'googlephotos': '**PRODUCT**',\n",
    "            'google': '**COMPANY**',\n",
    "            'alphabet': '**COMPANY**',\n",
    "            'androidpay': '**PRODUCT**',\n",
    "            'android': '**PRODUCT**',\n",
    "            'pixelxl': '**PRODUCT**',\n",
    "            'pixel': '**PRODUCT**',\n",
    "            'maps': '**PRODUCT**',\n",
    "            'youtube': '**PRODUCT**',\n",
    "            'chromecast': '**PRODUCT**',\n",
    "            'nexusx': '**PRODUCT**',\n",
    "            'nexusp': '**PRODUCT**',\n",
    "            'googletranslate': '**PRODUCT**',\n",
    "            'gboard': '**PRODUCT**'\n",
    "        },\n",
    "        'AAPL': {\n",
    "            'applemusic': '**PRODUCT**',\n",
    "            'applepay': '**PRODUCT**',\n",
    "            'apple': '**COMPANY**', \n",
    "            'macbook': '**PRODUCT**',\n",
    "            'iphone': '**PRODUCT**',\n",
    "            'ipad': '**PRODUCT**',\n",
    "            'ios': '**PRODUCT**',\n",
    "            'icloud': '**PRODUCT**',\n",
    "            'faceid': '**PRODUCT**',\n",
    "            'airpods': '**PRODUCT**',\n",
    "            'animoji': '**PRODUCT**',\n",
    "            'lightningpin': '**PRODUCT**',\n",
    "            'touchid': '**PRODUCT**'\n",
    "        },\n",
    "        'MSFT': {\n",
    "            'microsoft': '**COMPANY**',\n",
    "            'windows': '**PRODUCT**',\n",
    "            'onedrive': '**PRODUCT**',\n",
    "            'outlook': '**PRODUCT**',\n",
    "            'bing': '**PRODUCT**'\n",
    "        },\n",
    "        'AMD': {\n",
    "            'amd': '**COMPANY**',\n",
    "            'ryzen': '**PRODUCT**',\n",
    "            'radeon': '**PRODUCT**',\n",
    "            'rxvega': '**PRODUCT**'\n",
    "        },\n",
    "        'AMZN': {\n",
    "            'amazonfire':'**PRODUCT**',\n",
    "            'amazon': '**COMPANY**',\n",
    "            'echo': '**PRODUCT**',\n",
    "            'prime': '**PRODUCT**',\n",
    "            'alexa': '**PRODUCT**',\n",
    "            'firetv': '**PRODUCT**',\n",
    "            'amazonvisa': '**PRODUCT**'\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    save_headlines(headlines, kword_replacements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    consume_ticker_csv('AAPL', 'AAPL.csv')\n",
    "    consume_ticker_csv('AMZN', 'AMZN.csv')\n",
    "    consume_ticker_csv('AMD', 'AMD.csv')\n",
    "    consume_ticker_csv('GOOG', 'GOOG.csv')\n",
    "    consume_ticker_csv('MSFT', 'MSFT.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-gpu]",
   "language": "python",
   "name": "conda-env-tf-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
