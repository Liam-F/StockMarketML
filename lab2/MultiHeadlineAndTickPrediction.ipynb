{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from Database import db\n",
    " \n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, concatenate, SpatialDropout1D, GRU\n",
    "from keras.layers import Dense, Flatten, Embedding, LSTM, Activation, BatchNormalization, Dropout, Conv1D, MaxPooling1D\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "import keras.backend as K\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Options\n",
    "\n",
    "stocks      = ['AAPL', 'AMD', 'AMZN', 'GOOG', 'MSFT', 'INTC']\n",
    "all_sources = ['reddit', 'reuters', 'twitter', 'seekingalpha', 'fool', 'wsj', 'thestreet']\n",
    "\n",
    "sample_size = 5\n",
    "tick_window = 30\n",
    "max_length  = 50\n",
    "vocab_size  = None # Set by tokenizer\n",
    "emb_size    = 300\n",
    "\n",
    "epochs      = 120\n",
    "batch_size  = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def add_time(date, days):\n",
    "    \n",
    "    return (date + timedelta(days=days)).strftime('%Y-%m-%d')\n",
    "\n",
    "def clean(sentence):\n",
    "    \n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.replace('-', ' ').replace('_', ' ').replace('&', ' ')\n",
    "    sentence = re.sub('\\$?\\d+%?\\w?', 'numbertoken', sentence)\n",
    "    sentence = ''.join(c for c in sentence if c in \"abcdefghijklmnopqrstuvwxyz \")\n",
    "    sentence = re.sub('\\s+', ' ', sentence)\n",
    "    \n",
    "    return sentence.strip()\n",
    "\n",
    "def make_headline_to_effect_data():\n",
    "    \"\"\"\n",
    "    Headline -> Effect\n",
    "    \n",
    "    Creates essentially the X, Y data for the embedding model to use\n",
    "    when analyzing/encoding headlines. Returns a list of headlines and\n",
    "    a list of corresponding 'effects' which represent a change in the stock price.\n",
    "    \"\"\"\n",
    "    all_headlines, all_tick_hist, all_effects = [], [], []\n",
    "    \n",
    "    with db() as (conn, cur):\n",
    "        \n",
    "        for stock in stocks:\n",
    "            \n",
    "            ## Headline For Every Date ##\n",
    "            \n",
    "            cur.execute(\"SELECT DISTINCT date FROM headlines WHERE stock=? ORDER BY date ASC LIMIT 1\", [stock])\n",
    "            start_date = cur.fetchall()[0][0]\n",
    "            \n",
    "            cur.execute(\"SELECT DISTINCT date FROM ticks WHERE stock=? AND date >= ? ORDER BY date ASC\", [stock, start_date])\n",
    "            dates = [date[0] for date in cur.fetchall()]\n",
    "            \n",
    "            for date in tqdm_notebook(dates, desc=stock):\n",
    "                \n",
    "                ## Collect Headlines ##\n",
    "                \n",
    "                event_date = datetime.strptime(date, '%Y-%m-%d')\n",
    "                \n",
    "                cur.execute(\"SELECT date, source, rawcontent FROM headlines WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date DESC\", \n",
    "                            [stock, add_time(event_date, -14), date])\n",
    "                headlines = [(date, source, clean(content), (event_date - datetime.strptime(date, '%Y-%m-%d')).days) \n",
    "                                 for (date, source, content) in cur.fetchall() if content]\n",
    "                \n",
    "                if len(headlines) < sample_size:\n",
    "                    continue\n",
    "                    \n",
    "                ## Find corresponding tick data ## \n",
    "                \n",
    "                cur.execute(\"\"\"SELECT open, high, low, adjclose, volume FROM ticks WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date DESC\"\"\", \n",
    "                            [stock, \n",
    "                             add_time(event_date, -30 - tick_window), \n",
    "                             add_time(event_date, 0)])\n",
    "                \n",
    "                before_headline_ticks = cur.fetchall()[:tick_window]\n",
    "                \n",
    "                if len(before_headline_ticks) != tick_window:\n",
    "                    continue\n",
    "                \n",
    "                cur.execute(\"\"\"SELECT AVG(adjclose) FROM ticks WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date\"\"\", \n",
    "                            [stock, \n",
    "                             add_time(event_date, 1), \n",
    "                             add_time(event_date, 4)])\n",
    "                \n",
    "                after_headline_ticks = cur.fetchall()\n",
    "                \n",
    "                if len(after_headline_ticks) == 0:\n",
    "                    continue\n",
    "                \n",
    "                previous_tick = before_headline_ticks[0][3]\n",
    "                result_tick = after_headline_ticks[0][0]\n",
    "                \n",
    "                if not previous_tick or not result_tick:\n",
    "                    continue\n",
    "                \n",
    "                tick_hist = np.array(before_headline_ticks)\n",
    "                tick_hist -= np.mean(tick_hist, axis=0)\n",
    "                tick_hist /= np.std(tick_hist, axis=0)\n",
    "                \n",
    "                ## Create training example ##\n",
    "\n",
    "                probs = [1 / (headline[3] + 1) for headline in headlines]\n",
    "                probs /= np.sum(probs)\n",
    "                    \n",
    "                contents = [headline[2] for headline in headlines]\n",
    "\n",
    "                num_samples = len(contents) // sample_size\n",
    "                    \n",
    "                effect = [(result_tick - previous_tick) / previous_tick]\n",
    "\n",
    "                for i in range(num_samples):\n",
    "\n",
    "                    indexes = np.random.choice(np.arange(len(headlines)), sample_size, replace=False, p=probs)\n",
    "                    \n",
    "                    sample = [headlines[i] for i in indexes]\n",
    "\n",
    "                    all_headlines.append(sample)\n",
    "                    all_tick_hist.append(tick_hist)\n",
    "                    all_effects.append(effect)\n",
    "                    \n",
    "            break ## REMOVE BEFORE FLIGHT\n",
    "                    \n",
    "    return all_headlines, np.array(all_tick_hist), np.array(all_effects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def encode_sentences(headlines, tokenizer=None, max_length=100, vocab_size=100):\n",
    "    \"\"\"\n",
    "    Encoder\n",
    "    \n",
    "    Takes a list of headlines and converts them into vectors\n",
    "    \"\"\"\n",
    "    ## Encoding Sentences\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for example in headlines:\n",
    "        sentences.append(\" \".join([data[2] for data in example]))\n",
    "    \n",
    "    if not tokenizer:\n",
    "        \n",
    "        tokenizer = Tokenizer(num_words=vocab_size, filters='', lower=False) # Already PreProcessed\n",
    "    \n",
    "        tokenizer.fit_on_texts(sentences)\n",
    "    \n",
    "    encoded_headlines = tokenizer.texts_to_sequences(sentences)\n",
    "    \n",
    "    padded_headlines = pad_sequences(encoded_headlines, maxlen=max_length, padding='post')\n",
    "    \n",
    "    ## Encoding Meta Data\n",
    "    \n",
    "    # TODO\n",
    "    \n",
    "    return padded_headlines, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_embedding_matrix(tokenizer, pretrained_file='glove.840B.300d.txt'):\n",
    "    \n",
    "    embedding_matrix = np.zeros((vocab_size + 1, emb_size))\n",
    "    \n",
    "    if not pretrained_file:\n",
    "        return embedding_matrix, None\n",
    "    \n",
    "    ## Load Glove File (Super Slow) ##\n",
    "    \n",
    "    glove_db = dict()\n",
    "    \n",
    "    with open(os.path.join('..', 'data', pretrained_file), 'r', encoding=\"utf-8\") as glove:\n",
    "\n",
    "        for line in tqdm_notebook(glove, desc='Glove', total=2196017):\n",
    "\n",
    "            values = line.split(' ')\n",
    "            word = values[0].replace('-', '').lower()\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            glove_db[word] = coefs\n",
    "    \n",
    "    ## Set Embeddings ##\n",
    "    \n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        \n",
    "        embedding_vector = glove_db.get(word)\n",
    "        \n",
    "        if embedding_vector is not None:\n",
    "            \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix, glove_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdea18ad190943bc80f39e709bb0675e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='AAPL', max=2394), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd903dc06ef74245987d2bd41e173df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Glove', max=2196017), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    headlines, tick_hists, effects = make_headline_to_effect_data()\n",
    "    \n",
    "    encoded_headlines, toke = encode_sentences(headlines, max_length=max_length, vocab_size=vocab_size)\n",
    "    \n",
    "    vocab_size = len(toke.word_counts)\n",
    "    \n",
    "    emb_matrix, glove_db = get_embedding_matrix(toke)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-gpu]",
   "language": "python",
   "name": "conda-env-tf-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
