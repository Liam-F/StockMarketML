{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Flatten, Embedding, LSTM, Activation, BatchNormalization\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Options\n",
    "\n",
    "stocks = ['AAPL', 'GOOG']\n",
    "\n",
    "max_length = 100\n",
    "vocab_size = 500\n",
    "\n",
    "epochs = 120\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_tick_data(stocks):\n",
    "    \"\"\"\n",
    "    Tick Data\n",
    "    \n",
    "    This reads the high, lows, closes, etc. from data csv files\n",
    "    \"\"\"\n",
    "    history = {}\n",
    "    \n",
    "    for stock in stocks:\n",
    "        \n",
    "        history[stock] = {}\n",
    "        \n",
    "        with open(os.path.join('..', 'data', stock + '.csv'), 'r') as data:\n",
    "\n",
    "            for line in data:\n",
    "\n",
    "                if len(line) > 6 and \"Date\" not in line and \"null\" not in line:\n",
    "\n",
    "                    items = line.split(\",\")\n",
    "                    \n",
    "                    date = items[0]\n",
    "                    data = np.array(list(map(float, items[1:]))) # 0, 1, 2, 4, 5 -> OPEN HIGH LOW ADJ_CLOSE VOLUME\n",
    "                    \n",
    "                    history[stock][date] = data\n",
    "        \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_headline_data(stocks):\n",
    "    \"\"\"\n",
    "    Headline Data\n",
    "    \n",
    "    This reads the headlines from the headline csv file (created by CollectData)\n",
    "    \"\"\"\n",
    "    history = {}\n",
    "    \n",
    "    with open(os.path.join('..', 'data', \"_\".join(stocks) + '-headlines.csv'), 'r', encoding=\"utf8\") as data:\n",
    "        \n",
    "        for line in data:\n",
    "\n",
    "            if len(line) > 6:\n",
    "\n",
    "                stock, date, headlines = line.split(\",\")\n",
    "                \n",
    "                headlines = eval(headlines.strip().replace('@', ','))\n",
    "        \n",
    "                if not stock in history:\n",
    "                    \n",
    "                    history[stock] = {}\n",
    "                \n",
    "                history[stock][date] = headlines\n",
    "                \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_headline_to_effect_data(tick_data, head_data):\n",
    "    \"\"\"\n",
    "    Headline -> Effect\n",
    "    \n",
    "    Creates essentially the X, Y data for the embedding model to use\n",
    "    when analyzing/encoding headlines. Returns a list of headlines and\n",
    "    a list of corresponding 'effects' which represent a change in the stock price.\n",
    "    \"\"\"\n",
    "    all_headlines, effects = [], []\n",
    "    \n",
    "    for stock, dates in head_data.items():\n",
    "        \n",
    "        for date, headlines in dates.items():\n",
    "            \n",
    "            ## Find Matching tick data dates for headline dates ##\n",
    "            \n",
    "            event_date = datetime.strptime(date, '%Y-%m-%d') # The date `of` headline\n",
    "            effect_date = event_date + timedelta(days=1)     # The day after `affected` by headline\n",
    "            \n",
    "            for i in range(3):\n",
    "                if event_date.strftime('%Y-%m-%d') in tick_data[stock]:\n",
    "                    break\n",
    "                else:\n",
    "                    event_date -= timedelta(days=1)\n",
    "            else:\n",
    "                continue\n",
    "                    \n",
    "            for i in range(3):\n",
    "                if effect_date.strftime('%Y-%m-%d') in tick_data[stock]:\n",
    "                    break\n",
    "                else:\n",
    "                    effect_date += timedelta(days=1)\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            event_date = event_date.strftime('%Y-%m-%d')\n",
    "            effect_date = effect_date.strftime('%Y-%m-%d')\n",
    "            \n",
    "            ## Determine Effect ##\n",
    "            \n",
    "            if event_date in tick_data[stock] and effect_date in tick_data[stock]:\n",
    "                \n",
    "                tick_on = tick_data[stock][event_date]\n",
    "                tick_after = tick_data[stock][effect_date]\n",
    "                \n",
    "                if tick_after[3] >= tick_on[3]: # Compare Close Prices\n",
    "                    \n",
    "                    effect = [1., 0.]\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    effect = [0., 1.]\n",
    "                    \n",
    "                for source, headline in headlines.items():\n",
    "                    \n",
    "                    all_headlines.append(headline)\n",
    "                    effects.append(effect)\n",
    "                \n",
    "    return all_headlines, np.array(effects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def encode_sentences(sentences, max_length=16, vocab_size=100):\n",
    "    \"\"\"\n",
    "    Encoder\n",
    "    \n",
    "    Takes a list of headlines and converts them into vectors\n",
    "    \"\"\"\n",
    "    toke = Tokenizer(num_words=vocab_size)\n",
    "    \n",
    "    toke.fit_on_texts(sentences)\n",
    "    \n",
    "    encoded_headlines = toke.texts_to_sequences(sentences)\n",
    "    \n",
    "    padded_headlines = pad_sequences(encoded_headlines, maxlen=max_length, padding='post')\n",
    "    \n",
    "    return padded_headlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def split_data(X, Y, ratio, mix=True):\n",
    "    \"\"\"\n",
    "    Splits X/Y to Train/Test\n",
    "    \"\"\"\n",
    "    \n",
    "    if mix:\n",
    "        \n",
    "        X, Y = shuffle(X, Y)\n",
    "        \n",
    "    train_size = int(len(X) * ratio)\n",
    "    trainX, testX = X[:train_size], X[train_size:]\n",
    "    trainY, testY = Y[:train_size], Y[train_size:]\n",
    "    \n",
    "    return trainX, trainY, testX, testY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(vocab_size, 256, input_length=max_length))\n",
    "    \n",
    "    model.add(LSTM(100))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(100))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(100))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1892, 100) (473, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    tick_data = get_tick_data(stocks)\n",
    "    head_data = get_headline_data(stocks)\n",
    "    \n",
    "    headlines, effects = make_headline_to_effect_data(tick_data, head_data)\n",
    "    \n",
    "    encoded_headlines = encode_sentences(headlines, max_length=max_length, vocab_size=vocab_size)\n",
    "    \n",
    "    trainX, trainY, testX, testY = split_data(encoded_headlines, effects, .8)\n",
    "    \n",
    "    print(trainX.shape, testY.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    model = get_model()\n",
    "    \n",
    "    e_stopping = EarlyStopping(monitor='val_acc', patience=60)\n",
    "    \n",
    "    history = model.fit(trainX, \n",
    "                        trainY, \n",
    "                        epochs=epochs, \n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(testX, testY),\n",
    "                        verbose=0,\n",
    "                        callbacks=[e_stopping])\n",
    "    \n",
    "    plt.plot(np.log(history.history['loss']))\n",
    "    plt.plot(np.log(history.history['val_loss']))\n",
    "    plt.legend(['LogTrainLoss', 'LogTestLoss'])\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.legend(['TrainAcc', 'TestAcc'])\n",
    "    plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-gpu]",
   "language": "python",
   "name": "conda-env-tf-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
