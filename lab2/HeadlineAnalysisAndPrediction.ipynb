{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from Database import db\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, concatenate, SpatialDropout1D, GRU\n",
    "from keras.layers import Dense, Flatten, Embedding, LSTM, Activation, BatchNormalization, Dropout, Conv1D, MaxPooling1D\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Options\n",
    "\n",
    "stocks      = ['AAPL', 'AMD', 'AMZN', 'GOOG', 'MSFT']\n",
    "all_sources = ['reddit', 'reuters', 'twitter', 'seekingalpha']\n",
    "\n",
    "max_length  = 50\n",
    "vocab_size  = None # Set by tokenizer\n",
    "emb_size    = 300\n",
    "\n",
    "epochs     = 180\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_headline_to_effect_data():\n",
    "    \"\"\"\n",
    "    Headline -> Effect\n",
    "    \n",
    "    Creates essentially the X, Y data for the embedding model to use\n",
    "    when analyzing/encoding headlines. Returns a list of headlines and\n",
    "    a list of corresponding 'effects' which represent a change in the stock price.\n",
    "    \"\"\"\n",
    "    meta, headlines, effects = [], [], []\n",
    "    \n",
    "    with db() as (conn, cur):\n",
    "        \n",
    "        for stock in stocks:\n",
    "            \n",
    "            print(\"Fetching Stock...\" + stock)\n",
    "            \n",
    "            ## Go through all the headlines ##\n",
    "            \n",
    "            cur.execute(\"SELECT date, source, content FROM headlines WHERE stock=? AND LENGTH(content) >= 16\", [stock])\n",
    "            headline_query = cur.fetchall()\n",
    "            \n",
    "            for (date, source, content) in headline_query:\n",
    "                \n",
    "                event_date = datetime.strptime(date, '%Y-%m-%d') # The date of headline\n",
    "                \n",
    "                add_time = lambda e, days: (e + timedelta(days=days)).strftime('%Y-%m-%d')\n",
    "                \n",
    "                ## Find corresponding tick data ## \n",
    "                \n",
    "                cur.execute(\"\"\"SELECT adjclose FROM ticks WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date\"\"\", \n",
    "                            [stock, \n",
    "                             add_time(event_date, -3), \n",
    "                             add_time(event_date, 0)])\n",
    "                \n",
    "                before_headline_ticks = cur.fetchall()\n",
    "                \n",
    "                cur.execute(\"\"\"SELECT adjclose FROM ticks WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date\"\"\", \n",
    "                            [stock, \n",
    "                             add_time(event_date, 1), \n",
    "                             add_time(event_date, 4)])\n",
    "                \n",
    "                after_headline_ticks = cur.fetchall()\n",
    "                \n",
    "                ## Create training example ##\n",
    "                \n",
    "                if len(before_headline_ticks) > 0 and len(after_headline_ticks) > 0:\n",
    "                    \n",
    "                    previous_tick = before_headline_ticks[-1][0]\n",
    "                    result_tick = after_headline_ticks[0][0]\n",
    "                \n",
    "                    if result_tick > previous_tick:\n",
    "                        \n",
    "                        effect = [1., 0.]\n",
    "                        \n",
    "                    else:\n",
    "                        \n",
    "                        effect = [0., 1.]\n",
    "                        \n",
    "                    meta.append((source, event_date.weekday()))\n",
    "                    headlines.append(content)\n",
    "                    effects.append(effect)\n",
    "                    \n",
    "    return meta, headlines, np.array(effects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def encode_sentences(meta, sentences, tokenizer=None, max_length=100, vocab_size=100):\n",
    "    \"\"\"\n",
    "    Encoder\n",
    "    \n",
    "    Takes a list of headlines and converts them into vectors\n",
    "    \"\"\"\n",
    "    ## Encoding Sentences\n",
    "    \n",
    "    if not tokenizer:\n",
    "        \n",
    "        tokenizer = Tokenizer(num_words=vocab_size, filters='', lower=False) # Already Preprocessed\n",
    "    \n",
    "        tokenizer.fit_on_texts(sentences)\n",
    "    \n",
    "    encoded_headlines = tokenizer.texts_to_sequences(sentences)\n",
    "    \n",
    "    padded_headlines = pad_sequences(encoded_headlines, maxlen=max_length, padding='post')\n",
    "    \n",
    "    ## Encoding Meta Data\n",
    "    \n",
    "    # OneHot(Source) + OneHot(WeekDay)\n",
    "    \n",
    "    meta_matrix = np.zeros((len(sentences), len(all_sources) + 7))\n",
    "    index = 0\n",
    "    \n",
    "    for (source, weekday) in meta:\n",
    "        \n",
    "        meta_matrix[index, all_sources.index(source)] = 1\n",
    "        meta_matrix[index, len(all_sources) + weekday] = 1\n",
    "        \n",
    "        index += 1\n",
    "    \n",
    "    return meta_matrix, padded_headlines, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def split_data(X, X2, Y, ratio):\n",
    "    \"\"\"\n",
    "    Splits X/Y to Train/Test\n",
    "    \"\"\"\n",
    "    indexes = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indexes)\n",
    "    \n",
    "    X  = X[indexes]\n",
    "    X2 = X2[indexes]\n",
    "    Y  = Y[indexes]\n",
    "    \n",
    "    train_size = int(len(X) * ratio)\n",
    "    \n",
    "    trainX,  testX  = X[:train_size],  X[train_size:]\n",
    "    trainX2, testX2 = X2[:train_size], X2[train_size:]\n",
    "    trainY,  testY  = Y[:train_size],  Y[train_size:]\n",
    "    \n",
    "    return trainX, trainX2, trainY, testX, testX2, testY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_embedding_matrix(tokenizer, pretrained_file='glove.840B.300d.txt', purge=False):\n",
    "    \"\"\"Load Vectors from Glove File\"\"\"\n",
    "    print(\"Loading...WordVecs\")\n",
    "    \n",
    "    ## Load Glove File (Super Slow) ##\n",
    "    \n",
    "    glove_db = dict()\n",
    "    \n",
    "    with open(os.path.join('..', 'data', pretrained_file), 'r', encoding=\"utf-8\") as glove:\n",
    "\n",
    "        for line in glove:\n",
    "\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            glove_db[word] = coefs\n",
    "\n",
    "    print('Loaded Word Vectors...' + str(len(glove_db)))\n",
    "    \n",
    "    ## Set Embeddings ##\n",
    "    \n",
    "    embedding_matrix = np.zeros((vocab_size + 1, emb_size))\n",
    "    \n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        \n",
    "        embedding_vector = glove_db.get(word)\n",
    "        \n",
    "        if embedding_vector is not None:\n",
    "            \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "        elif purge:\n",
    "            \n",
    "            with db() as (conn, cur):\n",
    "                \n",
    "                cur.execute(\"SELECT 1 FROM specialwords WHERE word=?\", [word])\n",
    "                \n",
    "                if len(cur.fetchall()) == 0:\n",
    "                    \n",
    "                    print(\"Purge...\" + word)\n",
    "\n",
    "                    cur.execute(\"DELETE FROM headlines WHERE content LIKE ?\", [\"%\" + word + \"%\"])\n",
    "                    conn.commit()\n",
    "            \n",
    "    return embedding_matrix, glove_db\n",
    "\n",
    "def get_model(emb_matrix):\n",
    "    \n",
    "    ## Headline ##\n",
    "    \n",
    "    headline_input = Input(shape=(max_length,))\n",
    "    \n",
    "    emb = Embedding(vocab_size, emb_size, input_length=max_length, weights=[emb_matrix], trainable=True)(headline_input)\n",
    "    emb = SpatialDropout1D(.1)(emb)\n",
    "    \n",
    "    # conv = Conv1D(filters=64, kernel_size=5, padding='same', activation='selu')(emb)\n",
    "    # conv = MaxPooling1D(pool_size=3)(conv)\n",
    "    \n",
    "    text_rnn = LSTM(300, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)(emb)\n",
    "    text_rnn = Activation('relu')(text_rnn)\n",
    "    text_rnn = BatchNormalization()(text_rnn)\n",
    "    \n",
    "    text_rnn = LSTM(300, dropout=0.3, recurrent_dropout=0.3)(text_rnn)\n",
    "    text_rnn = Activation('relu')(text_rnn)\n",
    "    text_rnn = BatchNormalization()(text_rnn)\n",
    "    \n",
    "    ## Source ##\n",
    "    \n",
    "    meta_input = Input(shape=(len(all_sources) + 7,))\n",
    "    \n",
    "    ## Combined ##\n",
    "    \n",
    "    merged = concatenate([text_rnn, meta_input])\n",
    "    \n",
    "    dense_1 = Dense(300)(merged)\n",
    "    dense_1 = Activation('relu')(dense_1)\n",
    "    dense_1 = BatchNormalization()(dense_1)\n",
    "    dense_1 = Dropout(0.5)(dense_1)\n",
    "    \n",
    "    # dense_2 = Dense(300)(dense_1)\n",
    "    # dense_2 = Activation('relu')(dense_2)\n",
    "    # dense_2 = BatchNormalization()(dense_2)\n",
    "    # dense_2 = Dropout(0.5)(dense_2)\n",
    "    \n",
    "    dense_3 = Dense(200)(dense_1)\n",
    "    dense_3 = Activation('relu')(dense_3)\n",
    "    dense_3 = BatchNormalization()(dense_3)\n",
    "    dense_3 = Dropout(0.5)(dense_3)\n",
    "    \n",
    "    dense_4 = Dense(2)(dense_3)\n",
    "    out = Activation('softmax')(dense_4)\n",
    "    \n",
    "    model = Model(inputs=[headline_input, meta_input], outputs=out)\n",
    "    \n",
    "    optimizer = RMSprop(lr=0.001)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    meta, headlines, effects = make_headline_to_effect_data()\n",
    "    \n",
    "    encoded_meta, encoded_headlines, toke = encode_sentences(meta, \n",
    "                                                             headlines, \n",
    "                                                             max_length=max_length, \n",
    "                                                             vocab_size=vocab_size)\n",
    "    \n",
    "    vocab_size = len(toke.word_counts)\n",
    "    print(\"Found Words...\" + str(vocab_size))\n",
    "    \n",
    "    emb_matrix, glove_db = get_embedding_matrix(toke)\n",
    "    \n",
    "    trainX, trainX2, trainY, testX, testX2, testY = split_data(encoded_headlines, encoded_meta, effects, .85)\n",
    "    \n",
    "    print(trainX.shape, trainX2.shape, testY.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN MODEL\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    ## Save Tokenizer ##\n",
    "    \n",
    "    with open(os.path.join('..', 'models', 'toke.pkl'), 'wb') as toke_file:\n",
    "        pickle.dump(toke, toke_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    ## Create Model ##\n",
    "    \n",
    "    model = get_model(emb_matrix)\n",
    "    \n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(datetime.now().strftime(\"%Y,%m,%d-%H,%M,%S\")))\n",
    "    e_stopping = EarlyStopping(monitor='val_loss', patience=80)\n",
    "    checkpoint = ModelCheckpoint(os.path.join('..', 'models', 'media-headlines.h5'), \n",
    "                                 monitor='val_loss',\n",
    "                                 verbose=0,\n",
    "                                 save_best_only=True)\n",
    "    \n",
    "    ## Train ##\n",
    "    \n",
    "    history = model.fit([trainX, trainX2],\n",
    "                        trainY,\n",
    "                        epochs=epochs, \n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=([testX, testX2], testY),\n",
    "                        verbose=0,\n",
    "                        callbacks=[e_stopping, checkpoint, tensorboard])\n",
    "    \n",
    "    ## Display Train History ##\n",
    "    \n",
    "    plt.plot(np.log(history.history['loss']))\n",
    "    plt.plot(np.log(history.history['val_loss']))\n",
    "    plt.legend(['LogTrainLoss', 'LogTestLoss'])\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.legend(['TrainAcc', 'TestAcc'])\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST MODEL\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    ## Load Model For Manual Testing ##\n",
    "    \n",
    "    with open(os.path.join('..', 'models', 'toke.pkl'), 'rb') as toke_file:\n",
    "        toke = pickle.load(toke_file)\n",
    "    \n",
    "    model = load_model(os.path.join('..', 'models', 'media-headlines.h5'))\n",
    "    \n",
    "    ## Fake Unique Test Data ##\n",
    "    \n",
    "    test_sents = [\n",
    "        'the ceo of **COMPANY** was fired after selling a bad **PRODUCT**', \n",
    "        '**COMPANY** just released a **PRODUCT** thats better than every other company',\n",
    "        '**COMPANY**s **PRODUCT** killed a family of ducks in a sensor malfunction',\n",
    "        'the **COMPANY** team released a breakthrough in **PRODUCT** gaming'\n",
    "    ]\n",
    "    \n",
    "    ## Process ##\n",
    "    \n",
    "    encoded_meta, test_encoded, _ = encode_sentences([['reuters', 0], ['twitter', 1], ['reddit', 2], ['seekingalpha', 3]], \n",
    "                                                      test_sents, \n",
    "                                                      tokenizer=toke, \n",
    "                                                      max_length=max_length, \n",
    "                                                      vocab_size=vocab_size)\n",
    "    \n",
    "    predictions = model.predict([test_encoded, encoded_meta])\n",
    "    \n",
    "    ## Display Predictions ##\n",
    "    \n",
    "    for i in range(len(test_sents)):\n",
    "        \n",
    "        print(\"\")\n",
    "        print(test_sents[i])\n",
    "        print(predictions[i])\n",
    "        print(\"Stock Will Go Up\" if np.argmax(predictions[i]) == 0 else \"Stock Will Go Down\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST MODEL\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    ## Load Model For Manual Testing ##\n",
    "    \n",
    "    with open(os.path.join('..', 'models', 'toke.pkl'), 'rb') as toke_file:\n",
    "        toke = pickle.load(toke_file)\n",
    "    \n",
    "    model = load_model(os.path.join('..', 'models', 'media-headlines.h5'))\n",
    "    \n",
    "    ## **This Test May Overlap w/Train Data** ##\n",
    "    \n",
    "    current_date = '2018-02-05'\n",
    "    predict_date = '2018-02-06'\n",
    "    stock = 'GOOG'\n",
    "    \n",
    "    with db() as (conn, cur):\n",
    "        \n",
    "        ## Select Actual Stock Values ##\n",
    "        \n",
    "        cur.execute(\"\"\"SELECT adjclose FROM ticks WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date\"\"\", \n",
    "                    [stock, current_date, predict_date])\n",
    "        ticks = cur.fetchall()\n",
    "        \n",
    "        ## Find Headlines ##\n",
    "    \n",
    "        cur.execute(\"SELECT date, source, content FROM headlines WHERE date=? AND stock=?\", [current_date, stock])\n",
    "        headlines = cur.fetchall()\n",
    "        \n",
    "        ## Process ##\n",
    "        \n",
    "        meta, test_sents = [], []\n",
    "        \n",
    "        for (date, source, content) in headlines:\n",
    "            \n",
    "            meta.append([source, datetime.strptime(date, '%Y-%m-%d').weekday()])\n",
    "            test_sents.append(content)\n",
    "            \n",
    "        encoded_meta, test_encoded, _ = encode_sentences(meta, \n",
    "                                                         test_sents, \n",
    "                                                         tokenizer=toke, \n",
    "                                                         max_length=max_length,\n",
    "                                                         vocab_size=vocab_size)\n",
    "        \n",
    "        predictions = model.predict([test_encoded, encoded_meta])\n",
    "        \n",
    "        ## Display ##\n",
    "        \n",
    "        print(\"Using: \" + str(test_sents))\n",
    "        \n",
    "        print(\"Predicting Change Coef: \" +  str( round(np.mean(predictions[:, 0]) - .5, 2) * 2 ))\n",
    "        \n",
    "        print(\"Actual Stock Change: \" + str( round(ticks[-1][0] - ticks[0][0], 2) ))\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-gpu]",
   "language": "python",
   "name": "conda-env-tf-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
