{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from Database import db\n",
    " \n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, concatenate, SpatialDropout1D, GRU\n",
    "from keras.layers import Dense, Flatten, Embedding, LSTM, Activation, BatchNormalization, Dropout, Conv1D, MaxPooling1D\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "import keras.backend as K\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Options\n",
    "\n",
    "stocks      = ['AMD', 'INTC']\n",
    "all_sources = ['reddit', 'reuters', 'twitter', 'seekingalpha', 'fool', 'wsj', 'thestreet']\n",
    "\n",
    "tick_window = 25\n",
    "max_length  = 50\n",
    "vocab_size  = None # Set by tokenizer\n",
    "emb_size    = 300\n",
    "\n",
    "model_type  = 'regression'\n",
    "\n",
    "epochs      = 250\n",
    "batch_size  = 128\n",
    "\n",
    "test_cutoff = datetime(2018, 3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def add_time(date, days):\n",
    "    \n",
    "    return (date + timedelta(days=days)).strftime('%Y-%m-%d')\n",
    "\n",
    "def clean(sentence):\n",
    "    \n",
    "    if not sentence:\n",
    "        return \"\"\n",
    "    \n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.replace('-', ' ').replace('_', ' ').replace('&', ' ')\n",
    "    sentence = re.sub('\\$?\\d+%?\\w?', 'numbertoken', sentence)\n",
    "    sentence = sentence.replace('numbertokennumbertoken', 'numbertoken')\n",
    "    sentence = ''.join(c for c in sentence if c in \"abcdefghijklmnopqrstuvwxyz \")\n",
    "    sentence = re.sub('\\s+', ' ', sentence)\n",
    "    \n",
    "    return sentence.strip()\n",
    "\n",
    "def make_headline_to_effect_data():\n",
    "    \"\"\"\n",
    "    Headline -> Effect\n",
    "    \n",
    "    Creates essentially the X, Y data for the embedding model to use\n",
    "    when analyzing/encoding headlines. Returns a list of headlines and\n",
    "    a list of corresponding 'effects' which represent a change in the stock price.\n",
    "    \"\"\"\n",
    "    meta, headlines, tick_hists, effects, test_indices = [], [], [], [], []\n",
    "    \n",
    "    with db() as (conn, cur):\n",
    "        \n",
    "        for stock in stocks:\n",
    "            \n",
    "            print(\"Fetching Stock...\" + stock)\n",
    "            \n",
    "            ## Go through all the headlines ##\n",
    "            \n",
    "            cur.execute(\"SELECT date, source, rawcontent FROM headlines WHERE stock=?\", [stock])\n",
    "            headline_query = cur.fetchall()\n",
    "            \n",
    "            for (date, source, content) in headline_query:\n",
    "                \n",
    "                if not content:\n",
    "                    continue\n",
    "                \n",
    "                content = clean(content)\n",
    "                \n",
    "                if  not (5 <= content.count(' ') <= 40):\n",
    "                    continue\n",
    "                \n",
    "                event_date = datetime.strptime(date, '%Y-%m-%d') # The date of headline\n",
    "                \n",
    "                ## Find corresponding tick data ## \n",
    "                \n",
    "                cur.execute(\"\"\"SELECT open, high, low, adjclose, volume FROM ticks WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date DESC LIMIT 52\"\"\", \n",
    "                            [stock, \n",
    "                             add_time(event_date, -80), \n",
    "                             add_time(event_date, 0)])\n",
    "                \n",
    "                before_headline_ticks = cur.fetchall()\n",
    "                \n",
    "                if len(before_headline_ticks) < tick_window:\n",
    "                    continue\n",
    "                \n",
    "                cur.execute(\"\"\"SELECT adjclose FROM ticks WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date ASC LIMIT 1\"\"\", \n",
    "                            [stock, \n",
    "                             add_time(event_date, 1), \n",
    "                             add_time(event_date, 4)])\n",
    "                \n",
    "                after_headline_ticks = cur.fetchall()\n",
    "                \n",
    "                ## Create training example ##\n",
    "                \n",
    "                if len(after_headline_ticks) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                window_ticks = np.array(list(reversed(before_headline_ticks[:tick_window]))) # Flip so in chron. order\n",
    "                fifty_ticks = np.array(before_headline_ticks) # Use last 50 ticks to normalize\n",
    "                \n",
    "                previous_tick = before_headline_ticks[0][3]\n",
    "                result_tick = after_headline_ticks[0][0]\n",
    "                \n",
    "                if previous_tick and result_tick:\n",
    "                    \n",
    "                    window_ticks -= np.mean(fifty_ticks, axis=0)\n",
    "                    window_ticks /= np.std(fifty_ticks, axis=0)\n",
    "                    \n",
    "                    # Percent Diff (/ Normalization Constant)\n",
    "                    effect = [(result_tick - previous_tick) / previous_tick / 0.023]\n",
    "                                \n",
    "                    if event_date > test_cutoff: # Mark as Test Example\n",
    "                        test_indices.append(len(headlines))\n",
    "                        \n",
    "                    meta.append((source, event_date.weekday()))\n",
    "                    headlines.append(content)\n",
    "                    tick_hists.append(window_ticks)\n",
    "                    effects.append(effect)\n",
    "                    \n",
    "    return meta, headlines, np.array(tick_hists), np.array(effects), np.array(test_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def encode_sentences(meta, sentences, tokenizer=None, max_length=100, vocab_size=100):\n",
    "    \"\"\"\n",
    "    Encoder\n",
    "    \n",
    "    Takes a list of headlines and converts them into vectors\n",
    "    \"\"\"\n",
    "    ## Encoding Sentences\n",
    "    \n",
    "    if not tokenizer:\n",
    "        \n",
    "        tokenizer = Tokenizer(num_words=vocab_size, filters='', lower=False) # Already Preprocessed\n",
    "    \n",
    "        tokenizer.fit_on_texts(sentences)\n",
    "    \n",
    "    encoded_headlines = tokenizer.texts_to_sequences(sentences)\n",
    "    \n",
    "    padded_headlines = pad_sequences(encoded_headlines, maxlen=max_length, padding='post')\n",
    "    \n",
    "    ## Encoding Meta Data\n",
    "    \n",
    "    # OneHot(Source [reddit/twitter/reuters etc..]) + OneHot(WeekDay)\n",
    "    \n",
    "    meta_matrix = np.zeros((len(sentences), len(all_sources) + 7))\n",
    "    index = 0\n",
    "    \n",
    "    for (source, weekday) in meta:\n",
    "        \n",
    "        meta_matrix[index, all_sources.index(source)] = 1\n",
    "        meta_matrix[index, len(all_sources) + weekday] = 1\n",
    "        \n",
    "        index += 1\n",
    "    \n",
    "    return meta_matrix, padded_headlines, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def split_data(X, X2, X3, Y, test_indices):\n",
    "    \"\"\"\n",
    "    Splits X/Y to Train/Test\n",
    "    \"\"\"\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    train_indices = np.setdiff1d(indices, test_indices, assume_unique=True)\n",
    "    \n",
    "    trainX,  testX  = X[train_indices],  X[test_indices]\n",
    "    trainX2, testX2 = X2[train_indices], X2[test_indices]\n",
    "    trainX3, testX3 = X3[train_indices], X3[test_indices]\n",
    "    trainY,  testY  = Y[train_indices],  Y[test_indices]\n",
    "    \n",
    "    return trainX, trainX2, trainX3, trainY, testX, testX2, testX3, testY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_embedding_matrix(tokenizer, pretrained_file='glove.840B.300d.txt', purge=False):\n",
    "    \"\"\"Load Vectors from Glove File\"\"\"\n",
    "    print(\"Loading WordVecs...\")\n",
    "    \n",
    "    embedding_matrix = np.zeros((vocab_size + 1, emb_size))\n",
    "    \n",
    "    if not pretrained_file:\n",
    "        return embedding_matrix, None\n",
    "    \n",
    "    ## Load Glove File (Super Slow) ##\n",
    "    \n",
    "    glove_db = dict()\n",
    "    \n",
    "    with open(os.path.join('..', 'data', pretrained_file), 'r', encoding=\"utf-8\") as glove:\n",
    "\n",
    "        for line in glove:\n",
    "\n",
    "            values = line.split(' ')\n",
    "            word = values[0].replace('-', '').replace('_', '').lower()\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            \n",
    "            if word.isalpha():\n",
    "                glove_db[word] = coefs\n",
    "\n",
    "    print('Loaded WordVectors...' + str(len(glove_db)))\n",
    "    \n",
    "    ## Set Embeddings ##\n",
    "    \n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        \n",
    "        embedding_vector = glove_db.get(word)\n",
    "        \n",
    "        if embedding_vector is not None:\n",
    "            \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "        elif purge:\n",
    "            \n",
    "            with db() as (conn, cur):\n",
    "                \n",
    "                cur.execute(\"SELECT 1 FROM dictionary WHERE word=? AND stock=?\", [word, \"none\"])\n",
    "                \n",
    "                if len(cur.fetchall()) == 0:\n",
    "                    \n",
    "                    print(\"Purge...\" + word)\n",
    "\n",
    "                    cur.execute(\"DELETE FROM headlines WHERE content LIKE ?\", [\"%\" + word + \"%\"])\n",
    "                    conn.commit()\n",
    "            \n",
    "    return embedding_matrix, glove_db\n",
    "\n",
    "def correct_sign_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Accuracy of Being Positive or Negative\n",
    "    \"\"\"\n",
    "    diff = K.equal(y_true > 0, y_pred > 0)\n",
    "    \n",
    "    return K.mean(diff, axis=-1)\n",
    "\n",
    "def get_model(emb_matrix):\n",
    "    \n",
    "    ## Headline ##\n",
    "    \n",
    "    headline_input = Input(shape=(max_length,), name=\"headlines\")\n",
    "    \n",
    "    emb = Embedding(vocab_size + 1, emb_size, input_length=max_length, weights=[emb_matrix], trainable=True)(headline_input)\n",
    "    emb = SpatialDropout1D(.1)(emb)\n",
    "    \n",
    "    text_rnn = LSTM(200, recurrent_dropout=0.4, return_sequences=False)(emb)\n",
    "    text_rnn = Activation('selu')(text_rnn)\n",
    "    text_rnn = BatchNormalization()(text_rnn)\n",
    "    text_rnn = Dropout(0.5)(text_rnn)\n",
    "    \n",
    "    ## Ticks ##\n",
    "    \n",
    "    tick_input = Input(shape=(tick_window, 5), name=\"stockticks\")\n",
    "    \n",
    "    tick_conv = Conv1D(filters=128, kernel_size=6, padding='same', activation='selu')(tick_input)\n",
    "    tick_conv = MaxPooling1D(pool_size=2)(tick_conv)\n",
    "    tick_conv = Dropout(0.3)(tick_conv)\n",
    "    \n",
    "    tick_rnn = LSTM(360, dropout=0.3, recurrent_dropout=0.3, return_sequences=False)(tick_conv)\n",
    "    tick_rnn = Activation('selu')(tick_rnn)\n",
    "    tick_rnn = BatchNormalization()(tick_rnn)\n",
    "    \n",
    "    ## Meta ##\n",
    "    \n",
    "    meta_input = Input(shape=(len(all_sources) + 7,), name=\"metadata\")\n",
    "    \n",
    "    ## Combined ##\n",
    "    \n",
    "    merged = concatenate([text_rnn, tick_rnn, meta_input])\n",
    "    \n",
    "    final_dense = Dense(200)(merged)\n",
    "    final_dense = Activation('selu')(final_dense)\n",
    "    final_dense = BatchNormalization()(final_dense)\n",
    "    final_dense = Dropout(0.5)(final_dense)\n",
    "    \n",
    "    final_dense = Dense(200)(merged)\n",
    "    final_dense = Activation('selu')(final_dense)\n",
    "    final_dense = BatchNormalization()(final_dense)\n",
    "    final_dense = Dropout(0.5)(final_dense)\n",
    "    \n",
    "    final_dense = Dense(200)(merged)\n",
    "    final_dense = Activation('selu')(final_dense)\n",
    "    final_dense = BatchNormalization()(final_dense)\n",
    "    final_dense = Dropout(0.5)(final_dense)\n",
    "        \n",
    "    pred_dense = Dense(1)(final_dense)\n",
    "    out = pred_dense\n",
    "        \n",
    "    model = Model(inputs=[headline_input, tick_input, meta_input], outputs=out)\n",
    "    \n",
    "    model.compile(optimizer=RMSprop(lr=0.001), loss='mse', metrics=[correct_sign_acc])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    meta, headlines, tick_hists, effects, test_indices = make_headline_to_effect_data()\n",
    "    \n",
    "    encoded_meta, encoded_headlines, toke = encode_sentences(meta, \n",
    "                                                             headlines, \n",
    "                                                             max_length=max_length, \n",
    "                                                             vocab_size=vocab_size)\n",
    "    \n",
    "    vocab_size = len(toke.word_counts)\n",
    "    print(\"Found Words......\" + str(vocab_size))\n",
    "    \n",
    "    emb_matrix, glove_db = get_embedding_matrix(toke, purge=False)\n",
    "    \n",
    "    trainX, trainX2, trainX3, trainY, testX, testX2, testX3, testY = split_data(encoded_headlines, tick_hists, encoded_meta, effects, test_indices)\n",
    "    \n",
    "    print(trainX.shape, trainX2.shape, trainX3.shape, testY.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN MODEL\n",
    "\n",
    "if __name__ == \"__main__\":  \n",
    "    \n",
    "    ## Save Tokenizer ##\n",
    "    \n",
    "    with open(os.path.join('..', 'models', 'toke-tick.pkl'), 'wb') as toke_file:\n",
    "        pickle.dump(toke, toke_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    ## Create Model ##\n",
    "    \n",
    "    model = get_model(emb_matrix)\n",
    "    \n",
    "    monitor_mode = 'correct_sign_acc'\n",
    "    \n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(datetime.now().strftime(\"%Y,%m,%d-%H,%M,%S,tick,\" + model_type)))\n",
    "    e_stopping = EarlyStopping(monitor='val_loss', patience=50)\n",
    "    checkpoint = ModelCheckpoint(os.path.join('..', 'models', 'media-headlines-ticks-' + model_type + '.h5'), \n",
    "                                 monitor=monitor_mode,\n",
    "                                 verbose=0,\n",
    "                                 save_best_only=True)\n",
    "    \n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    \n",
    "    ## Train ##\n",
    "    \n",
    "    history = model.fit([trainX, trainX2, trainX3],\n",
    "                        trainY,\n",
    "                        epochs=epochs, \n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=([testX, testX2, testX3], testY),\n",
    "                        verbose=0,\n",
    "                        callbacks=[e_stopping, checkpoint, tensorboard])\n",
    "    \n",
    "    ## Display Train History ##\n",
    "    \n",
    "    plt.plot(np.log(history.history['loss']))\n",
    "    plt.plot(np.log(history.history['val_loss']))\n",
    "    plt.legend(['LogTrainLoss', 'LogTestLoss'])\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history[monitor_mode])\n",
    "    plt.plot(history.history['val_' + monitor_mode])\n",
    "    plt.legend(['TrainAcc', 'TestAcc'])\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict (TEST)\n",
    "\n",
    "def predict(stock, model=None, toke=None, current_date=None, predict_date=None, look_back=None, debug=False):\n",
    "    \n",
    "    import keras.metrics\n",
    "    keras.metrics.correct_sign_acc = correct_sign_acc\n",
    "    \n",
    "    if not model or not toke:\n",
    "        \n",
    "        with open(os.path.join('..', 'models', 'toke-tick.pkl'), 'rb') as toke_file:\n",
    "            toke = pickle.load(toke_file)\n",
    "    \n",
    "        model = load_model(os.path.join('..', 'models', 'media-headlines-ticks-' + model_type + '.h5'))\n",
    "        \n",
    "    vocab_size = len(toke.word_counts)\n",
    "        \n",
    "    if not current_date:\n",
    "        current_date = datetime.today()\n",
    "        \n",
    "    if not predict_date:\n",
    "        predict_date = current_date + timedelta(days=1)\n",
    "        \n",
    "    if not look_back:\n",
    "        look_back = 3\n",
    "    \n",
    "    pretick_date = add_time(current_date, -look_back)\n",
    "    \n",
    "    with db() as (conn, cur):\n",
    "        \n",
    "        ## Select Actual Stock Values ##\n",
    "                \n",
    "        cur.execute(\"\"\"SELECT open, high, low, adjclose, volume FROM ticks WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date DESC LIMIT 52\"\"\", \n",
    "                    [stock, \n",
    "                     add_time(current_date, -80), \n",
    "                     add_time(current_date, 0)])\n",
    "                \n",
    "        before_headline_ticks = cur.fetchall()\n",
    "\n",
    "        window_ticks = np.array(list(reversed(before_headline_ticks[:tick_window])))\n",
    "        fifty_ticks = np.array(before_headline_ticks)\n",
    "                    \n",
    "        window_ticks -= np.mean(fifty_ticks, axis=0)\n",
    "        window_ticks /= np.std(fifty_ticks, axis=0)\n",
    "        \n",
    "        cur.execute(\"\"\"SELECT adjclose FROM ticks WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date ASC LIMIT 1\"\"\", \n",
    "                   [stock, \n",
    "                    add_time(predict_date, 1), \n",
    "                    add_time(predict_date, 5)])\n",
    "        \n",
    "        after_headline_ticks = cur.fetchall()\n",
    "        \n",
    "        actual_current = before_headline_ticks[0][3]\n",
    "        \n",
    "        ## Find Headlines ##\n",
    "    \n",
    "        cur.execute(\"SELECT date, source, rawcontent FROM headlines WHERE date BETWEEN ? AND ? AND stock=?\", [pretick_date, current_date, stock])\n",
    "        headlines = cur.fetchall()\n",
    "        \n",
    "        ## Process ##\n",
    "        \n",
    "        meta, test_sents = [], []\n",
    "        \n",
    "        for (date, source, content) in headlines:\n",
    "            \n",
    "            meta.append([source, datetime.strptime(date, '%Y-%m-%d').weekday()])\n",
    "            test_sents.append(clean(content))\n",
    "            \n",
    "        if debug:\n",
    "            print(test_sents)\n",
    "            \n",
    "        encoded_meta, test_encoded, _ = encode_sentences(meta, \n",
    "                                                         test_sents, \n",
    "                                                         tokenizer=toke, \n",
    "                                                         max_length=max_length,\n",
    "                                                         vocab_size=vocab_size)\n",
    "        \n",
    "        tick_hists = np.array([window_ticks] * len(headlines))\n",
    "        \n",
    "        predictions = model.predict([test_encoded, tick_hists, encoded_meta])[:, 0]\n",
    "        \n",
    "        if debug:\n",
    "            print(predictions)\n",
    "        \n",
    "        prices = predictions * 0.023 * actual_current + actual_current\n",
    "        \n",
    "        return predictions, prices\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TEST] Metrics\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        actualY = (testY > 0) * 2 - 1\n",
    "        predictY = model.predict([testX, testX2, testX3])\n",
    "        \n",
    "        print(\"ROC\", roc_auc_score(actualY, predictY))\n",
    "        \n",
    "        print(confusion_matrix(actualY > 0, predictY > 0))\n",
    "        \n",
    "    except NameError:\n",
    "        \n",
    "        print(\"Test Data and Model Required!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# [TEST] Spot Testing\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    ## Options ##\n",
    "    \n",
    "    stock = 'AMD'\n",
    "    look_back = 3\n",
    "    current_date = '2018-04-01'\n",
    "    predict_date = '2018-04-02'\n",
    "    \n",
    "    ## Run ##\n",
    "    \n",
    "    predictions, prices = predict(stock, \n",
    "                                  current_date=datetime.strptime(current_date, '%Y-%m-%d'), \n",
    "                                  predict_date=datetime.strptime(predict_date, '%Y-%m-%d'), \n",
    "                                  look_back=look_back, debug=True)\n",
    "    \n",
    "    ## Find Actual Value ##\n",
    "     \n",
    "    with db() as (conn, cur):\n",
    "    \n",
    "        cur.execute(\"\"\"SELECT adjclose FROM ticks WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date ASC LIMIT 1\"\"\", \n",
    "                        [stock, \n",
    "                        add_time(datetime.strptime(predict_date, '%Y-%m-%d'), 0), \n",
    "                        add_time(datetime.strptime(predict_date, '%Y-%m-%d'), 6)])\n",
    "\n",
    "        after_headline_ticks = cur.fetchall()\n",
    "        try:\n",
    "            actual_result = after_headline_ticks[0][0]\n",
    "        except:\n",
    "            actual_result = -1\n",
    "            \n",
    "    ## Display ##\n",
    "            \n",
    "    parse = lambda num: str(round(num, 2))\n",
    "    \n",
    "    print(\"Predicting Change Coef: \" + parse(np.mean(predictions)))\n",
    "    print(\"Predicting Price: \" + parse(np.mean(prices)))\n",
    "    print(\"Actual Price: \" + parse(actual_result))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TEST] Range Test\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    ## Load Model For Manual Testing ##\n",
    "    \n",
    "    import keras.metrics\n",
    "    keras.metrics.correct_sign_acc = correct_sign_acc\n",
    "    \n",
    "    with open(os.path.join('..', 'models', 'toke-tick.pkl'), 'rb') as toke_file:\n",
    "        toke = pickle.load(toke_file)\n",
    "    \n",
    "    model = load_model(os.path.join('..', 'models', 'media-headlines-ticks-' + model_type + '.h5'))\n",
    "    \n",
    "    ## **This Test May Overlap w/Train Data** ##\n",
    "    \n",
    "    ## Settings ##\n",
    "    \n",
    "    stock = 'AMD'\n",
    "    start_date = '2017-02-25'\n",
    "    end_date = '2018-02-25'\n",
    "    \n",
    "    ## Run ##\n",
    "    \n",
    "    with db() as (conn, cur):\n",
    "        \n",
    "        cur.execute(\"\"\"SELECT date, adjclose FROM ticks WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date ASC\"\"\", \n",
    "                    [stock, \n",
    "                     datetime.strptime(start_date, '%Y-%m-%d'), \n",
    "                     datetime.strptime(end_date, '%Y-%m-%d')])\n",
    "        \n",
    "        real_ticks = cur.fetchall()\n",
    "        dates = sorted([ date for date, _ in real_ticks])\n",
    "        real_ticks = { date: close for (date, close) in real_ticks }\n",
    "        fake_ticks = { date: -1 for date in real_ticks }\n",
    "        \n",
    "    for date in dates:\n",
    "            \n",
    "        predict_date = datetime.strptime(date, '%Y-%m-%d')\n",
    "            \n",
    "        predictions, prices = predict(stock,                    \n",
    "                                      model=model,\n",
    "                                      toke=toke,\n",
    "                                      current_date=predict_date + timedelta(days=-1), \n",
    "                                      predict_date=predict_date, \n",
    "                                      look_back=3)\n",
    "            \n",
    "        fake_ticks[date] = np.mean(prices)\n",
    "        \n",
    "    real_ticks = np.array([real_ticks[date] for date in dates])\n",
    "    fake_ticks = np.array([fake_ticks[date] for date in dates])\n",
    "        \n",
    "    plt.plot(real_ticks)\n",
    "    plt.plot(fake_ticks)\n",
    "    plt.show()\n",
    "        \n",
    "    plt.plot(fake_ticks - real_ticks)\n",
    "    plt.show() \n",
    "    \n",
    "    acc_image = np.array([np.sign(fake_ticks[1:] - fake_ticks[:-1]) == np.sign(real_ticks[1:] - real_ticks[:-1])]) * 1.0\n",
    "    acc_image = acc_image.reshape((25, 10))\n",
    "\n",
    "    plt.imshow(acc_image, interpolation='none', cmap='RdBu')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Acc: \", np.mean(acc_image))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-gpu]",
   "language": "python",
   "name": "conda-env-tf-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
