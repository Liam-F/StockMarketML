{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from Database import db\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, concatenate, SpatialDropout1D, GRU\n",
    "from keras.layers import Dense, Flatten, Embedding, LSTM, Activation, BatchNormalization, Dropout, Conv1D, MaxPooling1D\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Options\n",
    "\n",
    "stocks      = ['AAPL', 'AMD', 'AMZN', 'GOOG', 'MSFT']\n",
    "all_sources = ['reddit', 'reuters', 'twitter', 'seekingalpha', 'fool']\n",
    "\n",
    "tick_window = 10\n",
    "max_length  = 50\n",
    "vocab_size  = None # Set by tokenizer\n",
    "emb_size    = 300\n",
    "\n",
    "model_type  = 'regression'\n",
    "\n",
    "epochs      = 1\n",
    "batch_size  = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_headline_to_effect_data():\n",
    "    \"\"\"\n",
    "    Headline -> Effect\n",
    "    \n",
    "    Creates essentially the X, Y data for the embedding model to use\n",
    "    when analyzing/encoding headlines. Returns a list of headlines and\n",
    "    a list of corresponding 'effects' which represent a change in the stock price.\n",
    "    \"\"\"\n",
    "    meta, headlines, tick_hists, effects = [], [], [], []\n",
    "    \n",
    "    with db() as (conn, cur):\n",
    "        \n",
    "        for stock in stocks:\n",
    "            \n",
    "            print(\"Fetching Stock...\" + stock)\n",
    "            \n",
    "            ## Go through all the headlines ##\n",
    "            \n",
    "            cur.execute(\"SELECT date, source, content FROM headlines WHERE stock=? AND LENGTH(content) >= 16\", [stock])\n",
    "            headline_query = cur.fetchall()\n",
    "            \n",
    "            for (date, source, content) in headline_query:\n",
    "                \n",
    "                event_date = datetime.strptime(date, '%Y-%m-%d') # The date of headline\n",
    "                \n",
    "                add_time = lambda e, days: (e + timedelta(days=days)).strftime('%Y-%m-%d')\n",
    "                \n",
    "                ## Find corresponding tick data ## \n",
    "                \n",
    "                cur.execute(\"\"\"SELECT open, high, low, adjclose, volume FROM ticks WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date DESC\"\"\", \n",
    "                            [stock, \n",
    "                             add_time(event_date, -10 - tick_window), \n",
    "                             add_time(event_date, 0)])\n",
    "                \n",
    "                before_headline_ticks = cur.fetchall()[:tick_window]\n",
    "                \n",
    "                if len(before_headline_ticks) != tick_window:\n",
    "                    continue\n",
    "                \n",
    "                cur.execute(\"\"\"SELECT AVG(adjclose) FROM ticks WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date\"\"\", \n",
    "                            [stock, \n",
    "                             add_time(event_date, 1), \n",
    "                             add_time(event_date, 6)])\n",
    "                \n",
    "                after_headline_ticks = cur.fetchall()\n",
    "                \n",
    "                ## Create training example ##\n",
    "                \n",
    "                if len(after_headline_ticks) > 0:\n",
    "                    \n",
    "                    tick_hist = np.array(before_headline_ticks)\n",
    "                    tick_hist -= np.mean(tick_hist, axis=0)\n",
    "                    tick_hist /= np.std(tick_hist, axis=0)\n",
    "                    \n",
    "                    previous_tick = before_headline_ticks[0][3]\n",
    "                    result_tick = after_headline_ticks[0][0]\n",
    "                    \n",
    "                    if model_type == 'regression':\n",
    "                        \n",
    "                        # Percent Diff (+Normalization Constant)\n",
    "                        effect = [(result_tick - previous_tick) / previous_tick / 0.0044]\n",
    "                    \n",
    "                    else:\n",
    "                \n",
    "                        if result_tick > previous_tick:\n",
    "\n",
    "                            effect = [1., 0.]\n",
    "\n",
    "                        else:\n",
    "\n",
    "                            effect = [0., 1.]\n",
    "                        \n",
    "                    meta.append((source, event_date.weekday()))\n",
    "                    headlines.append(content)\n",
    "                    tick_hists.append(tick_hist)\n",
    "                    effects.append(effect)\n",
    "                    \n",
    "                break\n",
    "                    \n",
    "    return meta, headlines, np.array(tick_hists), np.array(effects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def encode_sentences(meta, sentences, tokenizer=None, max_length=100, vocab_size=100):\n",
    "    \"\"\"\n",
    "    Encoder\n",
    "    \n",
    "    Takes a list of headlines and converts them into vectors\n",
    "    \"\"\"\n",
    "    ## Encoding Sentences\n",
    "    \n",
    "    if not tokenizer:\n",
    "        \n",
    "        tokenizer = Tokenizer(num_words=vocab_size, filters='', lower=False) # Already Preprocessed\n",
    "    \n",
    "        tokenizer.fit_on_texts(sentences)\n",
    "    \n",
    "    encoded_headlines = tokenizer.texts_to_sequences(sentences)\n",
    "    \n",
    "    padded_headlines = pad_sequences(encoded_headlines, maxlen=max_length, padding='post')\n",
    "    \n",
    "    ## Encoding Meta Data\n",
    "    \n",
    "    # OneHot(Source [reddit/twitter/reuters etc..]) + OneHot(WeekDay)\n",
    "    \n",
    "    meta_matrix = np.zeros((len(sentences), len(all_sources) + 7))\n",
    "    index = 0\n",
    "    \n",
    "    for (source, weekday) in meta:\n",
    "        \n",
    "        meta_matrix[index, all_sources.index(source)] = 1\n",
    "        meta_matrix[index, len(all_sources) + weekday] = 1\n",
    "        \n",
    "        index += 1\n",
    "    \n",
    "    return meta_matrix, padded_headlines, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def split_data(X, X2, X3, Y, ratio):\n",
    "    \"\"\"\n",
    "    Splits X/Y to Train/Test\n",
    "    \"\"\"\n",
    "    indexes = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indexes)\n",
    "    \n",
    "    X  = X[indexes]\n",
    "    X2 = X2[indexes]\n",
    "    X3 = X3[indexes]\n",
    "    Y  = Y[indexes]\n",
    "    \n",
    "    train_size = int(len(X) * ratio)\n",
    "    \n",
    "    trainX,  testX  = X[:train_size],  X[train_size:]\n",
    "    trainX2, testX2 = X2[:train_size], X2[train_size:]\n",
    "    trainX3, testX3 = X3[:train_size], X3[train_size:]\n",
    "    trainY,  testY  = Y[:train_size],  Y[train_size:]\n",
    "    \n",
    "    return trainX, trainX2, trainX3, trainY, testX, testX2, testX3, testY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_embedding_matrix(tokenizer, use_glove=True, pretrained_file='glove.840B.300d.txt', purge=False):\n",
    "    \"\"\"Load Vectors from Glove File\"\"\"\n",
    "    print(\"Loading WordVecs...\")\n",
    "    \n",
    "    embedding_matrix = np.zeros((vocab_size + 1, emb_size))\n",
    "    \n",
    "    if not use_glove:\n",
    "        return embedding_matrix, None\n",
    "    \n",
    "    ## Load Glove File (Super Slow) ##\n",
    "    \n",
    "    glove_db = dict()\n",
    "    \n",
    "    with open(os.path.join('..', 'data', pretrained_file), 'r', encoding=\"utf-8\") as glove:\n",
    "\n",
    "        for line in glove:\n",
    "\n",
    "            values = line.split(' ')\n",
    "            word = values[0].replace('-', '').lower()\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            glove_db[word] = coefs\n",
    "\n",
    "    print('Loaded WordVectors...' + str(len(glove_db)))\n",
    "    \n",
    "    ## Set Embeddings ##\n",
    "    \n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        \n",
    "        embedding_vector = glove_db.get(word)\n",
    "        \n",
    "        if embedding_vector is not None:\n",
    "            \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "        elif purge:\n",
    "            \n",
    "            with db() as (conn, cur):\n",
    "                \n",
    "                cur.execute(\"SELECT 1 FROM specialwords WHERE word=?\", [word])\n",
    "                \n",
    "                if len(cur.fetchall()) == 0:\n",
    "                    \n",
    "                    print(\"Purge...\" + word)\n",
    "\n",
    "                    cur.execute(\"DELETE FROM headlines WHERE content LIKE ?\", [\"%\" + word + \"%\"])\n",
    "                    conn.commit()\n",
    "            \n",
    "    return embedding_matrix, glove_db\n",
    "\n",
    "def correct_sign_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Accuracy of Being Positive or Negative\n",
    "    \"\"\"\n",
    "    diff = K.equal(y_true > 0, y_pred > 0)\n",
    "    \n",
    "    return K.mean(diff, axis=-1)\n",
    "\n",
    "def get_model(emb_matrix):\n",
    "    \n",
    "    ## Headline ##\n",
    "    \n",
    "    headline_input = Input(shape=(max_length,))\n",
    "    \n",
    "    emb = Embedding(vocab_size + 1, emb_size, input_length=max_length, weights=[emb_matrix], trainable=True)(headline_input)\n",
    "    emb = SpatialDropout1D(.2)(emb)\n",
    "    \n",
    "    # conv = Conv1D(filters=64, kernel_size=5, padding='same', activation='selu')(emb)\n",
    "    # conv = MaxPooling1D(pool_size=3)(conv)\n",
    "    \n",
    "    text_rnn = LSTM(200, dropout=0.3, recurrent_dropout=0.3, return_sequences=False)(emb)\n",
    "    text_rnn = Activation('selu')(text_rnn)\n",
    "    text_rnn = BatchNormalization()(text_rnn)\n",
    "    \n",
    "    # text_rnn = LSTM(300, dropout=0.3, recurrent_dropout=0.3)(text_rnn)\n",
    "    # text_rnn = Activation('relu')(text_rnn)\n",
    "    # text_rnn = BatchNormalization()(text_rnn)\n",
    "    \n",
    "    ## Ticks ##\n",
    "    \n",
    "    tick_input = Input(shape=(tick_window, 5))\n",
    "    \n",
    "    tick_rnn = LSTM(200, dropout=0.3, recurrent_dropout=0.3, return_sequences=False)(tick_input)\n",
    "    tick_rnn = Activation('selu')(tick_rnn)\n",
    "    tick_rnn = BatchNormalization()(tick_rnn)\n",
    "    \n",
    "    ## Source ##\n",
    "    \n",
    "    meta_input = Input(shape=(len(all_sources) + 7,))\n",
    "    \n",
    "    ## Combined ##\n",
    "    \n",
    "    merged = concatenate([text_rnn, tick_rnn, meta_input])\n",
    "    \n",
    "    final_dense = Dense(100)(merged)\n",
    "    final_dense = Activation('selu')(final_dense)\n",
    "    final_dense = BatchNormalization()(final_dense)\n",
    "    final_dense = Dropout(0.5)(final_dense)\n",
    "    \n",
    "    final_dense = Dense(100)(merged)\n",
    "    final_dense = Activation('selu')(final_dense)\n",
    "    final_dense = BatchNormalization()(final_dense)\n",
    "    final_dense = Dropout(0.5)(final_dense)\n",
    "    \n",
    "    if model_type == 'regression':\n",
    "        \n",
    "        pred_dense = Dense(1)(final_dense)\n",
    "        out = pred_dense\n",
    "        \n",
    "        model = Model(inputs=[headline_input, tick_input, meta_input], outputs=out)\n",
    "    \n",
    "        model.compile(optimizer=RMSprop(lr=0.001), loss='mse', metrics=[correct_sign_acc])\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        pred_dense = Dense(2)(final_dense)\n",
    "        out = Activation('softmax')(pred_dense)\n",
    "        \n",
    "        model = Model(inputs=[headline_input, tick_input, meta_input], outputs=out)\n",
    "    \n",
    "        model.compile(optimizer=RMSprop(lr=0.001), loss='categorical_crossentropy', metrics=['acc'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Stock...AAPL\n",
      "Fetching Stock...AMD\n",
      "Fetching Stock...AMZN\n",
      "Fetching Stock...GOOG\n",
      "Fetching Stock...MSFT\n",
      "Found Words......48\n",
      "Loading WordVecs...\n",
      "(4, 50) (4, 10, 5) (4, 12) (1, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    meta, headlines, tick_hists, effects = make_headline_to_effect_data()\n",
    "    \n",
    "    encoded_meta, encoded_headlines, toke = encode_sentences(meta, \n",
    "                                                             headlines, \n",
    "                                                             max_length=max_length, \n",
    "                                                             vocab_size=vocab_size)\n",
    "    \n",
    "    vocab_size = len(toke.word_counts)\n",
    "    print(\"Found Words......\" + str(vocab_size))\n",
    "    \n",
    "    emb_matrix, glove_db = get_embedding_matrix(toke, use_glove=False)\n",
    "    \n",
    "    trainX, trainX2, trainX3, trainY, testX, testX2, testX3, testY = split_data(encoded_headlines, tick_hists, encoded_meta, effects, .8)\n",
    "    \n",
    "    print(trainX.shape, trainX2.shape, trainX3.shape, testY.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2bfcd8b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2c55d2dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TRAIN MODEL\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    \n",
    "    ## Save Tokenizer ##\n",
    "    \n",
    "    with open(os.path.join('..', 'models', 'toke-tick.pkl'), 'wb') as toke_file:\n",
    "        pickle.dump(toke, toke_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    ## Create Model ##\n",
    "    \n",
    "    model = get_model(emb_matrix)\n",
    "    \n",
    "    if model_type == 'regression':\n",
    "        monitor_mode = 'correct_sign_acc'\n",
    "    else:\n",
    "        monitor_mode = 'val_acc'\n",
    "    \n",
    "    tensorboard = TensorBoard(log_dir=\"logs/{}\".format(datetime.now().strftime(\"%Y,%m,%d-%H,%M,%S,tick,\" + model_type)))\n",
    "    e_stopping = EarlyStopping(monitor=monitor_mode, patience=60)\n",
    "    checkpoint = ModelCheckpoint(os.path.join('..', 'models', 'media-headlines-ticks-' + model_type + '.h5'), \n",
    "                                 monitor=monitor_mode,\n",
    "                                 verbose=0,\n",
    "                                 save_best_only=True)\n",
    "    \n",
    "    ## Train ##\n",
    "    \n",
    "    history = model.fit([trainX, trainX2, trainX3],\n",
    "                        trainY,\n",
    "                        epochs=epochs, \n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=([testX, testX2, testX3], testY),\n",
    "                        verbose=0,\n",
    "                        callbacks=[e_stopping, checkpoint, tensorboard])\n",
    "    \n",
    "    ## Display Train History ##\n",
    "    \n",
    "    plt.plot(np.log(history.history['loss']))\n",
    "    plt.plot(np.log(history.history['val_loss']))\n",
    "    plt.legend(['LogTrainLoss', 'LogTestLoss'])\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history[monitor_mode])\n",
    "    plt.plot(history.history['val_' + monitor_mode])\n",
    "    plt.legend(['TrainAcc', 'TestAcc'])\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST MODEL\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "    \n",
    "#     ## Load Model For Manual Testing ##\n",
    "    \n",
    "#     import keras.metrics\n",
    "#     keras.metrics.correct_sign_acc = correct_sign_acc\n",
    "    \n",
    "#     with open(os.path.join('..', 'models', 'toke-tick.pkl'), 'rb') as toke_file:\n",
    "#         toke = pickle.load(toke_file)\n",
    "    \n",
    "#     model = load_model(os.path.join('..', 'models', 'media-headlines-ticks-' + model_type + '.h5'))\n",
    "    \n",
    "#     ## **This Test May Overlap w/Train Data** ##\n",
    "    \n",
    "#     pretick_date = '2018-02-12'\n",
    "#     current_date = '2018-02-14'\n",
    "#     predict_date = '2018-02-15'\n",
    "#     stock = 'AAPL'\n",
    "    \n",
    "#     with db() as (conn, cur):\n",
    "        \n",
    "#         ## Select Actual Stock Values ##\n",
    "        \n",
    "#         cur.execute(\"\"\"SELECT adjclose FROM ticks WHERE stock=? AND date BETWEEN ? AND ? ORDER BY date\"\"\", \n",
    "#                     [stock, current_date, predict_date])\n",
    "#         ticks = cur.fetchall()\n",
    "        \n",
    "#         ## Find Headlines ##\n",
    "    \n",
    "#         cur.execute(\"SELECT date, source, content FROM headlines WHERE date BETWEEN ? AND ? AND stock=?\", [pretick_date, current_date, stock])\n",
    "#         headlines = cur.fetchall()\n",
    "        \n",
    "#         ## Process ##\n",
    "        \n",
    "#         meta, test_sents = [], []\n",
    "        \n",
    "#         for (date, source, content) in headlines:\n",
    "            \n",
    "#             meta.append([source, datetime.strptime(date, '%Y-%m-%d').weekday()])\n",
    "#             test_sents.append(content)\n",
    "            \n",
    "#         encoded_meta, test_encoded, _ = encode_sentences(meta, \n",
    "#                                                          test_sents, \n",
    "#                                                          tokenizer=toke, \n",
    "#                                                          max_length=max_length,\n",
    "#                                                          vocab_size=vocab_size)\n",
    "        \n",
    "#         predictions = model.predict([test_encoded, encoded_meta])\n",
    "        \n",
    "#         ## Display ##\n",
    "        \n",
    "#         parse = lambda num: str(round(num, 2))\n",
    "        \n",
    "#         print(\"Using: \" + str(test_sents))\n",
    "        \n",
    "#         if model_type == 'regression':\n",
    "            \n",
    "#             print(\"Predicting Change Coef: \" +  parse(np.mean(predictions[:, 0])))\n",
    "#             print(\"Predicting Price: \" +  parse(np.mean(predictions[:, 0]) * 0.0044 * ticks[0][0] + ticks[0][0]))\n",
    "            \n",
    "#         else:\n",
    "        \n",
    "#             print(\"Predicting Change Coef: \" +  parse(np.mean(predictions[:, 0]) - .5))\n",
    "        \n",
    "#         print(\"Actual Stock: \" + parse(ticks[0][0]) + \" to \" + parse(ticks[-1][0]))\n",
    "#         print(\"Actual Stock Change: \" + parse(ticks[-1][0] - ticks[0][0]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TEST MODEL\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "     \n",
    "#     ## Load Model For Manual Testing ##\n",
    "    \n",
    "#     import keras.metrics\n",
    "#     keras.metrics.correct_sign_acc = correct_sign_acc\n",
    "     \n",
    "#     with open(os.path.join('..', 'models', 'toke-tick.pkl'), 'rb') as toke_file:\n",
    "#         toke = pickle.load(toke_file)\n",
    "    \n",
    "#     model = load_model(os.path.join('..', 'models', 'media-headlines-ticks-' + model_type + '.h5'))\n",
    "      \n",
    "#     ## Fake Unique Test Data ##\n",
    "    \n",
    "#     headlines = [\n",
    "#         \"**COMPANY** gains a ton of stock after creating **PRODUCT**\",\n",
    "#         \"**COMPANY** loses a ton of stock after killing **PRODUCT**\"\n",
    "#     ]\n",
    "    \n",
    "#     test_sents, meta = [], []\n",
    "    \n",
    "#     for headline in headlines:\n",
    "    \n",
    "#         for source in all_sources:\n",
    "\n",
    "#             for weekday in range(7):\n",
    "            \n",
    "#                 test_sents.append(headline)\n",
    "#                 meta.append([source, weekday])\n",
    "    \n",
    "#     ## Process ##\n",
    "    \n",
    "#     encoded_meta, test_encoded, _ = encode_sentences(meta, \n",
    "#                                                      test_sents, \n",
    "#                                                      tokenizer=toke, \n",
    "#                                                      max_length=max_length, \n",
    "#                                                      vocab_size=vocab_size)\n",
    "    \n",
    "#     predictions = model.predict([test_encoded, encoded_meta])\n",
    "    \n",
    "#     predictions = predictions.reshape((len(headlines), len(all_sources), 7))\n",
    "    \n",
    "#     ## Display Predictions ##\n",
    "    \n",
    "#     from matplotlib.colors import Normalize\n",
    "    \n",
    "#     for i, headline in enumerate(headlines):\n",
    "        \n",
    "#         plt.imshow(predictions[i], interpolation='none', cmap='PRGn', norm=Normalize(vmin=-2, vmax=2))\n",
    "#         plt.title(headline)\n",
    "#         plt.xlabel('Weekday')\n",
    "#         plt.ylabel('Source')\n",
    "#         plt.xticks(np.arange(7), list('MTWTFSS'))\n",
    "#         plt.yticks(np.arange(len(all_sources)), all_sources)\n",
    "#         plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-gpu]",
   "language": "python",
   "name": "conda-env-tf-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
