{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Users\\Shriv\\Anaconda3\\envs\\tf-cpu\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Setup (Imports)\n",
    "\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "\n",
    "import requests\n",
    "import random\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_reddit_news(subs, search_terms, limit=None, praw_config='StockMarketML'):\n",
    "    \n",
    "    from praw import Reddit\n",
    "    \n",
    "    reddit = Reddit(praw_config)\n",
    "\n",
    "    articles = defaultdict(list)\n",
    "    \n",
    "    used = []\n",
    "    \n",
    "    for term in search_terms:\n",
    "\n",
    "        for submission in reddit.subreddit('+'.join(subs)).search(term, limit=limit):\n",
    "            \n",
    "            if submission.title not in used:\n",
    "                \n",
    "                used.append(submission.title)\n",
    "\n",
    "                articles[datetime.fromtimestamp(submission.created).strftime('%Y-%m-%d')].append(submission.title)\n",
    "        \n",
    "    return articles\n",
    "\n",
    "def get_reuters_news(stock, limit=400):\n",
    "    \n",
    "    articles = defaultdict(list)\n",
    "    \n",
    "    pattern_headline = re.compile('<h2>\\s*(<a [\\S]*\\s*>)?(.+?)(<\\/a>)?\\s*<\\/h2>')\n",
    "    \n",
    "    date_current = datetime.now()\n",
    "    \n",
    "    while limit > 0:\n",
    "        \n",
    "        text = requests.get('http://www.reuters.com/finance/stocks/company-news/{}?date={}'.format(stock, date_current.strftime('%m%d%Y'))).text\n",
    "        \n",
    "        for match in pattern_headline.finditer(text):\n",
    "            \n",
    "            headline = match.group(2)\n",
    "            \n",
    "            headline = re.sub('[A-Z][A-Z\\d\\s]{5,}\\-', '', headline)\n",
    "            \n",
    "            articles[date_current.strftime('%Y-%m-%d')].append(headline)\n",
    "        \n",
    "            limit -= 1\n",
    "        \n",
    "        date_current -= timedelta(days=1)\n",
    "        \n",
    "    return articles\n",
    "\n",
    "def save_headlines(stock, sources, force_one_per_day=False):\n",
    "    \n",
    "    articles = defaultdict(list)\n",
    "    \n",
    "    for source in sources:\n",
    "        \n",
    "        for date in source:\n",
    "            \n",
    "            articles[date].extend(source[date])\n",
    "            \n",
    "    with open(os.path.join('data', stock + '-headlines.csv'), 'w', encoding=\"utf-8\") as headline_file:\n",
    "        \n",
    "        for date in sorted(articles):\n",
    "            \n",
    "            current_articles = articles[date]\n",
    "            \n",
    "            if force_one_per_day:\n",
    "                \n",
    "                current_articles = [random.choice(current_articles)]\n",
    "        \n",
    "            headline_file.write(\"{},{}\\n\".format(date, \"@@\".join(current_articles).replace(',', '')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    save_headlines('AAPL', \n",
    "                   [get_reddit_news(['apple', 'ios', 'AAPL', 'news'], ['apple', 'iphone', 'ipad', 'ios']), \n",
    "                    get_reuters_news('AAPL.O')], \n",
    "                   force_one_per_day=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def process_raw_text(text):\n",
    "\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    \n",
    "    cleaned = list(map(lambda w: w.lower(), words))\n",
    "\n",
    "    return \" \".join(cleaned)\n",
    "\n",
    "def convert_headlines_to_vectors(stock, create_model=True):\n",
    "    \n",
    "    def read_headline_file():\n",
    "        \n",
    "        with open(os.path.join('data', stock + '-headlines.csv'), 'r', encoding=\"utf-8\") as headline_file:\n",
    "        \n",
    "            for line in headline_file:\n",
    "            \n",
    "                if len(line) > 6:\n",
    "                \n",
    "                    date, headlines = line.split(',')\n",
    "                \n",
    "                    yield date, map(lambda x: x.strip(), headlines.split(\"@@\"))\n",
    "    \n",
    "    if create_model:\n",
    "        \n",
    "        i = 0\n",
    "        \n",
    "        headlines_corpus = []\n",
    "        \n",
    "        for date, headlines in read_headline_file():\n",
    "            \n",
    "            for headline in headlines:\n",
    "                \n",
    "                if headline not in headlines_corpus:\n",
    "                \n",
    "                    headlines_corpus.append(LabeledSentence(process_raw_text(headline), tags=['headline_' + str(i)]))\n",
    "                \n",
    "                    i += 1\n",
    "        \n",
    "        doc_model = Doc2Vec(headlines_corpus, size=100, window=5, min_count=3, workers=4)\n",
    "        doc_model.save(os.path.join('models', stock + '-headlines-doc2vec.model'))\n",
    "    \n",
    "    doc_model = Doc2Vec.load(os.path.join('models', stock + '-headlines-doc2vec.model'))\n",
    "    \n",
    "    with open(os.path.join('data', stock + '-headlines-vectors.csv'), 'w', encoding=\"utf-8\") as headline_vectors:\n",
    "        \n",
    "        i = 0\n",
    "        \n",
    "        used = []\n",
    "        \n",
    "        for date, headlines in read_headline_file(): #TODO file read not needed\n",
    "        \n",
    "            for headline in headlines:\n",
    "                \n",
    "                if headline not in used:\n",
    "                    \n",
    "                    used.append(headline)\n",
    "                \n",
    "                    vector = doc_model.docvecs[i]\n",
    "                \n",
    "                    vector = str(list(vector))\n",
    "                \n",
    "                    headline_vectors.write(\"{},{}\\n\".format(date, vector))\n",
    "                \n",
    "                i += 1\n",
    "    \n",
    "    return doc_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    convert_headlines_to_vectors('AAPL')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-cpu]",
   "language": "python",
   "name": "conda-env-tf-cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
