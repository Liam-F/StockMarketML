{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Users\\Shriv\\Anaconda3\\envs\\tf-cpu\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Setup (Imports)\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def process_raw_text(text):\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text_processed = tokenizer.tokenize(text)\n",
    "    \n",
    "    text_processed = [word.lower() for word in text_processed if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "    porter_stemmer = PorterStemmer()\n",
    "\n",
    "    text_processed = [porter_stemmer.stem(word) for word in text_processed]\n",
    "\n",
    "    return \" \".join(text_processed)\n",
    "\n",
    "def convert_sentences_to_vector(sentences):\n",
    "    \n",
    "    sentences = list(map(process_raw_text, sentences))\n",
    "    \n",
    "    dictionary = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        \n",
    "        dictionary.append(sentence.split(' '))\n",
    "        \n",
    "    word_model = Word2Vec(dictionary, size=100, window=5, min_count=1, workers=4)\n",
    "    word_model.save(os.path.join('models', 'word2vec.model'))\n",
    "    \n",
    "    vector = [[word_model.wv[word] for word in sentence.split(' ')] for sentence in sentences]\n",
    "    \n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_reddit_news(subs, search_term, limit=None, praw_config='StockMarketML'):\n",
    "    \n",
    "    from praw import Reddit\n",
    "    \n",
    "    reddit = Reddit(praw_config)\n",
    "\n",
    "    articles = defaultdict(list)\n",
    "\n",
    "    for submission in reddit.subreddit('+'.join(subs)).search(search_term, limit=limit):\n",
    "    \n",
    "        articles[datetime.fromtimestamp(submission.created).strftime('%m/%d/%Y')].append(submission.title)\n",
    "        \n",
    "    return articles\n",
    "\n",
    "def get_reuters_news(stock, limit=200):\n",
    "    \n",
    "    articles = defaultdict(list)\n",
    "    \n",
    "    pattern_headline = re.compile('<h2>\\s*(<a [\\S]*\\s*>)?(.+?)(<\\/a>)?\\s*<\\/h2>')\n",
    "    \n",
    "    date_current = datetime.now()\n",
    "    \n",
    "    while limit > 0:\n",
    "        \n",
    "        text = requests.get('http://www.reuters.com/finance/stocks/company-news/{}?date={}'.format(stock, date_current.strftime('%m%d%Y'))).text\n",
    "        \n",
    "        for match in pattern_headline.finditer(text):\n",
    "            \n",
    "            headline = match.group(2)\n",
    "            \n",
    "            articles[date_current.strftime('%m/%d/%Y')].append(headline)\n",
    "        \n",
    "            limit -= 1\n",
    "        \n",
    "        date_current -= timedelta(days=1)\n",
    "        \n",
    "    return articles\n",
    "\n",
    "def get_yahoo_finance_news(suburl=\"\", limit=1): # TODO FIX\n",
    "    \n",
    "    pattern_headline = re.compile('<u class=\"StretchedBox\" data-reactid=\"\\d+\"><\\/u><!-- react-text: \\d+ -->(.+?)<!-- \\/react-text --><\\/a><\\/h3>')\n",
    "    \n",
    "    url = \"https://finance.yahoo.com/\" + suburl \n",
    "    \n",
    "    while limit > 0:\n",
    "        \n",
    "        text = requests.get(url).text\n",
    "        \n",
    "        for match in pattern_headline.finditer(text):\n",
    "            \n",
    "            headline = match.group(1)\n",
    "            \n",
    "            print(headline)\n",
    "        \n",
    "        limit -= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get_reddit_news(['news', 'apple', 'ios', 'AAPL'], 'apple')\n",
    "# get_reuters_news('AAPL.O')\n",
    "# get_yahoo_finance_news('tech/apple')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-cpu]",
   "language": "python",
   "name": "conda-env-tf-cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
