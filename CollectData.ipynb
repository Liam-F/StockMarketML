{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Users\\Shriv\\Anaconda3\\envs\\tf-cpu\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Setup (Imports)\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from praw import Reddit\n",
    "\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_raw_text(text):\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    text_processed = tokenizer.tokenize(text)\n",
    "    \n",
    "    text_processed = [word.lower() for word in text_processed if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "    porter_stemmer = PorterStemmer()\n",
    "\n",
    "    text_processed = [porter_stemmer.stem(word) for word in text_processed]\n",
    "\n",
    "    return \" \".join(text_processed)\n",
    "\n",
    "def convert_sentences_to_vector(sentences):\n",
    "    \n",
    "    sentences = list(map(process_raw_text, sentences))\n",
    "    \n",
    "    dictionary = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        \n",
    "        dictionary.append(sentence.split(' '))\n",
    "        \n",
    "    word_model = Word2Vec(dictionary, size=100, window=5, min_count=1, workers=4)\n",
    "    word_model.save(os.path.join('models', 'word2vec.model'))\n",
    "    \n",
    "    vector = [[word_model.wv[word] for word in sentence.split(' ')] for sentence in sentences]\n",
    "    \n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reddit = Reddit('StockMarketML')\n",
    "\n",
    "articles = defaultdict(list)\n",
    "sentences = []\n",
    "\n",
    "for submission in reddit.subreddit('news+apple+ios+AAPL').search('apple', limit=None):\n",
    "    \n",
    "    articles[datetime.fromtimestamp(submission.created).strftime('%Y-%m-%d')].append(submission.title)\n",
    "    \n",
    "    sentences.append(submission.title)\n",
    "    \n",
    "print(convert_sentences_to_vector(sentences)[0])\n",
    "    \n",
    "with open(os.path.join('data', 'reddit.csv'), 'w') as redditfile:\n",
    "    \n",
    "    for date, sents in articles.items():\n",
    "        \n",
    "        data = str(sents).encode(\"utf-8\")\n",
    "    \n",
    "        redditfile.write(date + \", \" + str(data)[1:] + \"\\n\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-cpu]",
   "language": "python",
   "name": "conda-env-tf-cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
