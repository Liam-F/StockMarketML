{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\Users\\Shriv\\Anaconda3\\envs\\tf-cpu\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Setup (Imports)\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_reddit_news(subs, search_term, limit=None, praw_config='StockMarketML'):\n",
    "    \n",
    "    from praw import Reddit\n",
    "    \n",
    "    reddit = Reddit(praw_config)\n",
    "\n",
    "    articles = defaultdict(list)\n",
    "\n",
    "    for submission in reddit.subreddit('+'.join(subs)).search(search_term, limit=limit):\n",
    "    \n",
    "        articles[datetime.fromtimestamp(submission.created).strftime('%Y-%m-%d')].append(submission.title)\n",
    "        \n",
    "    return articles\n",
    "\n",
    "def get_reuters_news(stock, limit=200):\n",
    "    \n",
    "    articles = defaultdict(list)\n",
    "    \n",
    "    pattern_headline = re.compile('<h2>\\s*(<a [\\S]*\\s*>)?(.+?)(<\\/a>)?\\s*<\\/h2>')\n",
    "    \n",
    "    date_current = datetime.now()\n",
    "    \n",
    "    while limit > 0:\n",
    "        \n",
    "        text = requests.get('http://www.reuters.com/finance/stocks/company-news/{}?date={}'.format(stock, date_current.strftime('%m%d%Y'))).text\n",
    "        \n",
    "        for match in pattern_headline.finditer(text):\n",
    "            \n",
    "            headline = match.group(2)\n",
    "            \n",
    "            headline = re.sub('[A-Z][A-Z\\d\\s]{5,}\\-', '', headline)\n",
    "            \n",
    "            articles[date_current.strftime('%Y-%m-%d')].append(headline)\n",
    "        \n",
    "            limit -= 1\n",
    "        \n",
    "        date_current -= timedelta(days=1)\n",
    "        \n",
    "    return articles\n",
    "\n",
    "def save_headlines(stock, sources):\n",
    "    \n",
    "    articles = defaultdict(list)\n",
    "    \n",
    "    for source in sources:\n",
    "        \n",
    "        for date in source:\n",
    "            \n",
    "            articles[date].extend(source[date])\n",
    "            \n",
    "    with open(os.path.join('data', stock + '-headlines.csv'), 'w', encoding=\"utf-8\") as headline_file:\n",
    "        \n",
    "        for date in sorted(articles):\n",
    "        \n",
    "            headline_file.write(\"{},{}\\n\".format(date, \"@@\".join(articles[date]).replace(',', '')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def process_raw_text(text):\n",
    "\n",
    "    text_processed = RegexpTokenizer(r'\\w+').tokenize(text)\n",
    "    \n",
    "    text_processed = [word.lower() for word in text_processed if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "    porter_stemmer = PorterStemmer()\n",
    "\n",
    "    text_processed = [porter_stemmer.stem(word) for word in text_processed]\n",
    "\n",
    "    return \" \".join(text_processed)\n",
    "\n",
    "def convert_headlines_to_vectors(stock, create_model=True):\n",
    "    \n",
    "    def read_headline_file():\n",
    "        \n",
    "        with open(os.path.join('data', stock + '-headlines.csv'), 'r', encoding=\"utf-8\") as headline_file:\n",
    "        \n",
    "            for line in headline_file:\n",
    "            \n",
    "                if len(line) > 6:\n",
    "                \n",
    "                    date, headlines = line.split(',')\n",
    "                \n",
    "                    yield date, headlines.split(\"@@\")\n",
    "    \n",
    "    if create_model:\n",
    "    \n",
    "        dictionary = []\n",
    "    \n",
    "        for date, headlines in read_headline_file():\n",
    "        \n",
    "            for headline in headlines:\n",
    "                \n",
    "                dictionary.append(process_raw_text(headline).split(' '))\n",
    "        \n",
    "        word_model = Word2Vec(dictionary, size=100, window=5, min_count=3, workers=4)\n",
    "        word_model.save(os.path.join('models', stock + '-headlines-word2vec.model'))\n",
    "    \n",
    "    word_model = Word2Vec.load(os.path.join('models', stock + '-headlines-word2vec.model'))\n",
    "    \n",
    "    with open(os.path.join('data', stock + '-headlines-vectors.csv'), 'w', encoding=\"utf-8\") as headline_vectors:\n",
    "        \n",
    "        for date, headlines in read_headline_file():\n",
    "        \n",
    "            for headline in headlines:\n",
    "                \n",
    "                vector = [list(word_model.wv[w]) for w in process_raw_text(headline).split(' ') if w in word_model.wv]\n",
    "                \n",
    "                headline_vectors.write(\"{},{}\\n\".format(date, vector))\n",
    "    \n",
    "    return word_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    save_headlines('AAPL', [get_reddit_news(['news', 'apple', 'ios', 'AAPL'], 'apple'), get_reuters_news('AAPL.O')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    convert_headlines_to_vectors('AAPL')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-cpu]",
   "language": "python",
   "name": "conda-env-tf-cpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
